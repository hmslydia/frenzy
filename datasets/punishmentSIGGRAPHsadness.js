tweets = {
    "siggraph0": {
        "time": 0, 
        "html": "<b>Optimizing Locomotion Controllers Using Biologically-Based Actuators and Objectives</b><br><br> #Authors: Jack M. Wang, Samuel R. Hamner, Scott L. Delp, and Vladlen Koltun<br><br> #Abstract: We present a technique for automatically synthesizing walking and running controllers for physically-simulated 3D humanoid characters. The sagittal hip, knee, and ankle degrees-of-freedom are actuated using a set of eight Hill-type musculotendon models in each leg, with biologically-motivated control laws. The parameters of these control laws are set by an optimization procedure that satisfies a number of locomotion task terms while minimizing a biological model of metabolic energy expenditure. We show that the use of biologically-based actuators and objectives measurably increases the realism of gaits generated by locomotion controllers that operate without the use of motion capture data, and that metabolic energy expenditure provides a simple and unifying measurement of effort that can be used for both walking and running control optimization. #Image <img src='http://vladlen.info/wp-content/uploads/2012/04/biolocomotion21.png' width='95%'>", 
        "id": "siggraph0", 
        "parent": "none", 
        "creator": "siggraph"
    }, 
    "siggraph1": {
        "time": 1, 
        "html": "<b>Soft Body Locomotion</b><br><br> #Authors: Jie Tan, Greg Turk, and C. Karen Liu<br><br> #Abstract: We present a physically-based system to simulate and control the locomotion of soft body characters without skeletons. We use the finite element method to simulate the deformation of the soft body, and we instrument a character with muscle fibers to allow it to actively control its shape. To perform locomotion, we use a variety of intuitive controls such as moving a point on the character, specifying the center of mass or the angular momentum, and maintaining balance. These controllers yield an objective function that is passed to our optimization solver, which handles convex quadratic program with linear complementarity constraints. This solver determines the new muscle fiber lengths, and moreover it determines whether each point of contact should remain static, slide, or lift away from the floor. Our system can automatically find an appropriate combination of muscle contractions that enables a soft character to fulfill various locomotion tasks, including walking, jumping, crawling, rolling and balancing. #Image <img src='http://homes.cs.washington.edu/~katlyn/siggraph/Soft%20Body%20Locomotion.png' width='95%'>", 
        "id": "siggraph1", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph2" : {
        "time": 2,
        "html": "<b>Continuous Character Control with Low-Dimensional Embeddings</b><br><br> #Authors: Sergey Levine, Jack M. Wang, Alexis Haraux, Zoran Popović, and Vladlen Koltun <br><br> #Abstract: Interactive, task-guided character controllers must be agile and responsive to user input, while retaining the flexibility to be readily authored and modified by the designer. Central to a method’s ease of use is its capacity to synthesize character motion for novel situations without requiring excessive data or programming effort. In this work, we present a technique that animates characters performing user-specified tasks by using a probabilistic motion model, which is trained on a small number of artist-provided animation clips. The method uses a low-dimensional space learned from the example motions to continuously control the character’s pose to accomplish the desired task. By controlling the character through a reduced space, our method can discover new transitions, tractably precompute a control policy, and avoid low quality poses. #Image <img src='http://vladlen.info/wp-content/uploads/2012/04/low-dimensional-controllers1.png' width='95%'>",
        "id": "siggraph2",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph3" : {
        "time": 3,
        "html": "<b> Schelling Points on 3D Surface Meshes </b><br><br> #Authors: Xiaobai Chen, Abulhair Saparov, Bill Pang, Thomas Funkhouser <br><br> #Abstract: This paper investigates 'Schelling points' on 3D meshes, feature points selected by people in a pure coordination game due to their salience. To collect data for this investigation, we designed an online experiment that asked people to select points on 3D surfaces that they expect will be selected by other people. We then analyzed properties of the selected points, finding that: 1) Schelling point sets are usually highly symmetric, and 2) local curvature properties (e.g., Gauss curvature) are most helpful for identifying obvious Schelling points (tips of protrusions), but 3) global properties (e.g., segment centeredness, proximity to a symmetry axis, etc.) are required to explain more subtle features. Based on these observations, we use regression analysis to combine multiple properties into an analytical model that predicts where Schelling points are likely to be on new meshes. We find that this model benefits from a variety of surface properties, particularly when training data comes from examples in the same object class. #Image <img src='http://gfx.cs.princeton.edu/pubs/Chen_2012_SPO/Density.png' width='95%'>",
        "id": "siggraph3",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph4" : {
        "time": 4,
        "html": "<b> Variational Mesh Decomposition </b><br><br> #Authors: J. Zhang, J. Zheng, C. Wu, J. Cai  <br><br> #Abstract: This paper presents a variational mesh decomposition algorithm that can efficiently partition a mesh into a prescribed number of segments. The algorithm extends the Mumford-Shah model to 3D meshes, which simultaneously handles segmentation and boundary smoothing. The efficiency is achieved by solving the Mumford-Shah model through a saddle-point problem that is solved by a fast primal-dual method. A pre-process step is also proposed to determine the number of segments that the mesh should be decomposed into. By incorporating this pre-processing step, the proposed algorithm can automatically segment a mesh into meaningful parts. Furthermore, user interaction is allowed by incorporating user's inputs into the variational model to reflect user's special intention. Experimental results show that the proposed algorithm outperforms competitive segmentation methods when evaluated on the Princeton Segmentation Benchmark. #Image <img src='http://homes.cs.washington.edu/~katlyn/siggraph/Variational%20Mesh%20Decomposition.png' width='95%'>",
        "id": "siggraph4",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph5" : {
        "time": 5,
        "html": "<b> Sketch-Based Shape Retrieval </b><br><br> #Authors: Mathias Eitz, Ronald Richter, Tamy Boubekeur, Kristian Hildebrand and Marc Alexa <br><br> #Abstract: We develop a system for 3D object retrieval based on sketched feature lines as input. For objective evaluation, we collect a large number of query sketches from human users that are related to an existing data base of objects. The sketches turn out to be generally quite abstract with large local and global deviations from the original shape. Based on this observation, we decide to use a bag-of-features approach over computer generated line drawings of the objects. We develop a targeted feature transform based on Gabor filters for this system. We can show objectively that this transform is better suited than other approaches from the literature developed for similar tasks. Moreover, we demonstrate how to optimize the parameters of our, as well as other approaches, based on the gathered sketches. In the resulting comparison, our approach is significantly better than any other system described so far. #Image <img src='http://cybertron.cg.tu-berlin.de/eitz/projects/sbsr/teaser_sbsr.jpg' width='95%'>",
        "id": "siggraph5",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph6" : {
        "time": 6,
        "html": "<b> Specular Reflection from Woven Cloth </b><br><br> #Authors: Piti Irawan and Steve Marschner <br><br> #Abstract: The appearance of a particular fabric is produced by variations in both large-scale reflectance and small-scale texture as the viewing and illumination angles change across the surface. This paper presents a study of the reflectance and texture of woven cloth that aims to identify and model important optical features of cloth appearance. New measurements are reported for a range of fabrics including natural and synthetic fibers as well as staple and filament yarns. A new scattering model for woven cloth is introduced that describes the reflectance and the texture based on an analysis of specular reflection from the fibers. Unlike data-based models, our procedural model doesn’t require image data. It can handle a wide range of fabrics using a small set of physically meaningful parameters that describe the characteristics of the fibers, the geometry of the yarns, and the pattern of the weave. The model is validated against the measurements and evaluated by comparisons to high-resolution video of the real fabrics and to BTF models of two of the fabrics. #Image <img src='http://www.cs.cornell.edu/~srm/images/weave-patterns.jpg' width='95%'>",
        "id": "siggraph6",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph7" : {
        "time": 7,
        "html": "<b> DRAPE: DRessing Any PErson </b><br><br> #Authors: Peng Guan, Loretta Reiss, David Abraham Hirshberg, Alex Weiss, Michael J. Black <br><br> #Abstract: Clothed virtual characters in varied sizes and shapes are necessary for film, gaming, and on-line fashion applications. Dressing such characters is a significant bottleneck, requiring manual effort to design clothing, position it on the body, and simulate its physical deformation. To address this, we have developed a complete system for animating realistic clothing on synthetic bodies of any shape and pose without manual intervention. DRAPE is learned from standard 2D clothing designs simulated on 3D avatars with varying shape and pose. Once learned, DRAPE adapts to different body shapes and poses without the redesign of clothing patterns; this effectively creates infinitely-sized clothing. A key contribution is that the method is automatic. In particular, animators do not need to place the cloth pieces in appropriate positions to dress an avatar. #Image <img src='http://ps.is.tue.mpg.de/uploads/project_photo/image/1/research_photo_research_photo_DRAPEfigure.jpg?1361759417' width='95%'>",
        "id": "siggraph7",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph8" : {
        "time": 8,
        "html": "<b> Design Preserving Garment Transfer </b><br><br> #Authors: Rémi Brouet, Alla Sheffer, Laurence Boissieux, Marie-Paule Cani <br><br> #Abstract: We present a fully automatic method for design-preserving transfer of garments between characters with different body shapes. For real life garments, such transfer is performed through a knowledge intensive and time consuming process, known as pattern grading. Our first contribution is to reformulate the criteria used in professional pattern-grading as a set of geometric requirement, respectively expressing shape or design preservation, proportionality, and fit. We then propose a fully automatic garment transfer algorithm which satisfies all of these criteria while ensuring the physical plausibility of result. #Image <img src='http://hal.inria.fr/docs/00/73/50/76/IMG/01_WomanToDaughter.jpg' width='95%'>",
        "id": "siggraph8",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph9" : {
        "time": 9,
        "html": "<b> Stitch Meshes </b><br><br> #Authors: Cem YukselJonathan M. KaldorDoug L. JamesSteve Marschner <br><br> #Abstract: Recent yarn-based simulation techniques permit realistic and efficient dynamic simulation of knitted clothing, but producing the required yarn-level models remains a challenge. The lack of practical modeling techniques significantly limits the diversity and complexity of knitted garments that can be simulated. We propose a new modeling technique that builds yarn-level models of complex knitted garments for virtual characters. We start with a polygonal model that represents the large-scale surface of the knitted cloth. Using this mesh as an input, our interactive modeling tool produces a finer mesh representing the layout of stitches in the garment, which we call the stitch mesh. By manipulating this mesh and assigning stitch types to its faces, the user can replicate a variety of complicated knitting patterns. The curve model representing the yarn is generated from the stitch mesh, then the final shape is computed by a yarn-level physical simulation that locally relaxes the yarn into realistic shape while preserving global shape of the garment and avoiding ``yarn pull-through,'' thereby producing valid yarn geometry suitable for dynamic simulation. Using our system, we can efficiently create yarn-level models of knitted clothing with a rich variety of patterns that would be completely impractical to model using traditional techniques. We show a variety of example knitting patterns and full-scale garments produced using our system. #Image <img src='http://www.cemyuksel.com/research/stitchmeshes/sheep_small.jpg' width='95%'>",
        "id": "siggraph9",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph10" : {
        "time": 10,
        "html": "<b> decoupling algorithms from schedules for easy optimization of image processing pipelines </b><br><br> #Authors: Jonathan Ragan-Kelley Andrew Adams Sylvain Paris Marc Levoy Saman Amarasinghe Frédo Durand <br><br> #Abstract: Using existing programming tools, writing high-performance image processing code requires sacrificing readability, portability, and modularity. We argue that this is a consequence of conflating what computations define the algorithm, with decisions about storage and the order of computation. We refer to these latter two concerns as the schedule, including choices of tiling, fusion, recomputation vs. storage, vectorization, and parallelism. We propose a representation for feed-forward imaging pipelines that separates the algorithm from its schedule, enabling high-performance without sacrificing code clarity. This decoupling simplifies the algorithm specification: images and intermediate buffers become functions over an infinite integer domain, with no explicit storage or boundary conditions. Imaging pipelines are compositions of functions. Programmers separately specify scheduling strategies for the various functions composing the algorithm, which allows them to efficiently explore different optimizations without changing the algorithmic code. We demonstrate the power of this representation by expressing a range of recent image processing applications in an embedded domain specific language called Halide, and compiling them for ARM, x86, and GPUs. Our compiler targets SIMD units, multiple cores, and complex memory hierarchies. We demonstrate that it can handle algorithms such as a camera raw pipeline, the bilateral grid, fast local Laplacian filtering, and image segmentation. The algorithms expressed in our language are both shorter and faster than state-of-the-art implementations. #Image <img src='http://people.csail.mit.edu/jrk/halide12/teaser.png' width='95%'>",
        "id": "siggraph10",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph11" : {
        "time": 11,
        "html": "<b> Adaptive Manifolds for Real-Time High-Dimensional Filtering </b><br><br> #Authors: Eduardo S. L. Gastal and Manuel M. Oliveira <br><br> #Abstract: We present a technique for performing high-dimensional filtering of images and videos in real time. Our approach produces high-quality results and accelerates filtering by computing the filter's response at a reduced set of sampling points, and using these for interpolation at all N input pixels. We show that for a proper choice of these sampling points, the total cost of the filtering operation is linear both in N and in the dimension d of the space in which the filter operates. As such, ours is the first high-dimensional filter with such a complexity. We present formal derivations for the equations that define our filter, as well as for an algorithm to compute the sampling points. This provides a sound theoretical justification for our method and for its properties. The resulting filter is quite flexible, being capable of producing responses that approximate either standard Gaussian, bilateral, or non-local-means filters. Such flexibility also allows us to demonstrate the first hybrid Euclidean-geodesic filter that runs in a single pass. Our filter is faster and requires less memory than previous approaches, being able to process a 10-Megapixel full-color image at 50 fps on modern GPUs. We illustrate the effectiveness of our approach by performing a variety of tasks ranging from edge-aware color filtering in 5-D, noise reduction (using up to 147 dimensions), single-pass hybrid Euclidean-geodesic filtering, and detail enhancement, among others. #Image <img src='http://inf.ufrgs.br/~eslgastal/AdaptiveManifolds/images/adaptive_manifolds_for_real-time_high-dimensional_filtering.jpg' width='95%'>",
        "id": "siggraph11",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph12" : {
        "time": 12,
        "html": "<b> Practical temporal consistency for image-based graphics applications </b><br><br> #Authors: Manuel Lang, Oliver Wang, Tunc Aydin, Aljoscha Smolic, Markus Gross <br><br> #Abstract: We present an efficient and simple method for introducing temporal consistency to a large class of optimization driven image-based computer graphics problems. Our method extends recent work in edge-aware filtering, approximating costly global regularization with a fast iterative joint filtering operation. Using this representation, we can achieve tremendous efficiency gains both in terms of memory requirements and running time. This enables us to process entire shots at once, taking advantage of supporting information that exists across far away frames, something that is difficult with existing approaches due to the computational burden of video data. Our method is able to filter along motion paths using an iterative approach that simultaneously uses and estimates per-pixel optical flow vectors. We demonstrate its utility by creating temporally consistent results for a number of applications including optical flow, disparity estimation, colorization, scribble propagation, sparse data up-sampling, and visual saliency computation. #Image <img src='http://zurich.disneyresearch.com/~malang/projects/FeatureFlow/teaser.png' width='95%'>",
        "id": "siggraph12",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph13" : {
        "time": 13,
        "html": "<b> 3D Imaging Spectroscopy for Measuring Hyperspectral Patterns on Solid Objects </b><br><br> #Authors: Min H. Kim, Todd Alan Harvey, David S. Kittle, Holly Rushmeier, Julie Dorsey, Richard O. Prum, David J. Brady <br><br> #Abstract: Sophisticated methods for true spectral rendering have been developed in computer graphics to produce highly accurate images. In addition to traditional applications in visualizing appearance, such methods have potential applications in many areas of scientific study. In particular, we are motivated by the application of studying avian vision and appearance. An obstacle to using graphics in this application is the lack of reliable input data. We introduce an end-to-end measurement system for capturing spectral data on 3D objects. We present the modification of a recently developed hyperspectral imager to make it suitable for acquiring such data in a wide spectral range at high spectral and spatial resolution. We capture four megapixel images, with data at each pixel from the near-ultraviolet (359 nm) to near-infrared (1,003 nm) at 12 nm spectral resolution. We fully characterize the imaging system, and document its accuracy. This imager is integrated into a 3D scanning system to enable the measurement of the diffuse spectral reflectance and fluorescence of specimens. We demonstrate the use of this measurement system in the study of the interplay between the visual capabilities and appearance of birds. We show further the use of the system in gaining insight into artifacts from geology and cultural heritage. #Image <img src='http://vclab.kaist.ac.kr/siggraph2012/3dis_teaser.png' width='95%'>",
        "id": "siggraph13",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph14" : {
        "time": 14,
        "html": "<b> Fast High-Resolution Appearance Editing Using Superimposed Projections </b><br><br> #Authors: D. Aliaga, Y. Yeung, A. Law, B. Sajadi, A. Majumder <br><br> #Abstract: The quality of an appearance edit is directly related to the resolution achievable by the projectors available. Various factors affect a projector's resolution and radiant power, including the projector's distance to the object and the orientation of the projector to the object. In this work, we analyze the appearance editing setup and projector pixel interaction to improve the quality of our appearance edits. Our projectors are set up such that their fields of projection are superimposed on the object's surface, and we model projector pixels as elliptical Gaussians. Thus, we require inverting a complex multi-projector light transport matrix via a constrained optimization when modeling the projector pixel interaction. We also introduce a technique to swap quickly between different appearances without having to recalculate full compensation images.  #Image <img src='http://wiki.cs.purdue.edu/cgvlab/lib/exe/fetch.php?w=700&h=&cache=cache&media=wiki:projects:appearance_editing:iae-pedestal-pumpkin.png' width='95%'>",
        "id": "siggraph14",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph15" : {
        "time": 15,
        "html": "<b> Printing Spatially Varying Reflectance for Reproducing HDR Images </b><br><br> #Authors: Yue Dong, Xin Tong, Fabio Pellaciniyz, Baining Guo <br><br> #Abstract: We present a solution for viewing high dynamic range (HDR) images with spatially-varying distributions of glossy materials printed on reflective media. Our method exploits the appearance variations of the glossy materials in the angular domain to display the input HDR image at different exposures. As viewers change the print orientation or lighting directions, the print gradually varies its appearance to display the image content from the darkest to the brightest levels. Our solution is based on a commercially available printing system and is fully automatic. Given the input HDR image and the BRDFs of a set of available inks, our method computes the optimal exposures of the HDR image for all viewing conditions and the optimal ink combinations for all pixels by minimizing the difference of their appearances under all viewing conditions. We demonstrate the effectiveness of our method with print samples generated from different inputs and visualized under different viewing and lighting conditions. #Image <img src='http://homes.cs.washington.edu/~katlyn/siggraph/Printing%20Spacially.png' width='95%'>",
        "id": "siggraph15",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph16" : {
        "time": 16,
        "html": "<b> Primal-Dual Coding to Probe Light Transport </b><br><br> #Authors: Matthew O'Toole, Ramesh Raskar, and Kiriakos N. Kutulakos. <br><br> #Abstract: We present primal-dual coding, a photography technique that enables direct fine-grain control over which light paths contribute to a photo. We achieve this by projecting a sequence of patterns onto the scene while the sensor is exposed to light. At the same time, a second sequence of patterns, derived from the first and applied in lockstep, modulates the light received at individual sensor pixels. We show that photography in this regime is equivalent to a matrix probing operation in which the elements of the scene's transport matrix are individually re-scaled and then mapped to the photo. This makes it possible to directly acquire photos in which specific light transport paths have been blocked, attenuated or enhanced. We show captured photos for several scenes with challenging light transport effects, including specular inter-reflections, caustics, diffuse inter-reflections and volumetric scattering. A key feature of primal-dual coding is that it operates almost exclusively in the optical domain: our results consist of directly-acquired, unprocessed RAW photos or differences between them. #Image <img src='http://homes.cs.washington.edu/~katlyn/siggraph/Primal%20Dual.png' width='95%'>",
        "id": "siggraph16",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph17" : {
        "time": 17,
        "html": "<b> Printing Reflectance Functions </b><br><br> #Authors: Tom Malzbender, Ramin Samadani, Steven Scher, Adam Crume, Douglas Dunn, and James Davis <br><br> #Abstract: The reflectance function of a scene point captures the appearance of that point as a function of lighting direction. We present an approach to printing the reflectance functions of an object or scene so that its appearance is modified correctly as a function of the lighting conditions when viewing the print. For example, such a .photograph. of a statue printed with our approach appears to cast shadows to the right when the .photograph. is illuminated from the left. Viewing the same print with lighting from the right will cause the statue.s shadows to be cast to the left. Beyond shadows, all effects due to the lighting variation, such as Lambertian shading, specularity, and inter-reflection can be reproduced. We achieve this ability by geometrically and photometrically controlling specular highlights on the surface of the print. For a particular viewpoint, arbitrary reflectance functions can be built up at each pixel by controlling only the specular highlights and avoiding significant diffuse reflections. Our initial binary prototype uses halftoning to approximate continuous grayscale reflectance functions. #Image <img src='http://graphics.soe.ucsc.edu/prf/Teaser.jpg' width='95%'>",
        "id": "siggraph17",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph18" : {
        "time": 18,
        "html": "<b> Eyecatch: Simulating Visuomotor Coordination For Object Interception </b><br><br> #Authors: Sang Hoon Yeo, Martin Lesmana, Debanga R. Neog, Dinesh K. Pai<br><br> #Abstract: We present a novel framework for animating human characters performing fast visually guided tasks, such as catching a ball. The main idea is to consider the coordinated dynamics of sensing and movement. Based on experimental evidence about such behaviors, we propose a generative model that constructs interception behavior online, using discrete submovements directed by uncertain visual estimates of target movement. An important aspect of this framework is that eye movements are included as well, and play a central role in coordinating movements of the head, hand, and body. We show that this framework efficiently generates plausible movements and generalizes well to novel scenarios. #Image <img src='https://www.cs.ubc.ca/sites/cs/files/imagecache/feature_image/research/images/thumbnail.jpg' width='95%'>",
        "id": "siggraph18",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph19" : {
        "time": 19,
        "html": "<b> Discovery of Complex Behaviors through Contact-Invariant Optimization </b><br><br> #Authors: Igor Mordatch, Emanuel Todorov, Zoran Popović <br><br> #Abstract: We present a motion synthesis framework capable of producing a wide variety of important human behaviors that have rarely been studied, including getting up from the ground, crawling, climbing, moving heavy objects, acrobatics (hand-stands in particular), and various cooperative actions involving two characters and their manipulation of the environment. Our framework is not speciﬁc to humans, but applies to characters of arbitrary morphology and limb conﬁguration. The approach is fully automatic and does not require domain knowledge speciﬁc to each behavior. It also does not require pre-existing examples or motion capture data. At the core of our framework is the contact-invariant optimization (CIO) method we introduce here. It enables simultaneous optimization of contact and behavior. This is done by augmenting the search space with scalar variables that indicate whether a potential contact should be active in a given phase of the movement. These auxiliary variables affect not only the cost function but also the dynamics (by enabling and disabling contact forces), and are optimized together with the movement trajectory. Additional innovations include a continuation scheme allowing helper forces at the potential contacts rather than the torso, as well as a feature-based model of physics which is particularly well-suited to the CIO framework. We expect that CIO can also be used with a full physics model, but leave that extension for future work. #Image <img src='http://homes.cs.washington.edu/~mordatch/cio/cio_thumb.jpg' width='95%'>",
        "id": "siggraph19",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph10" : {
        "time": 20,
        "html": "<b> Bilinear Spatiotemporal Basis Models </b><br><br> #Authors: Ijaz Akhter , Tomas Simon, Sohaib Khan, Iain Matthews, Yaser Sheikh <br><br> #Abstract: A variety of dynamic objects, such as faces, bodies, and cloth, are represented in computer graphics as a collection of moving spatial landmarks. Spatiotemporal data is inherent in a number of graphics applications including animation, simulation, and object and camera tracking. The principal modes of variation in the spatial geometry of objects are typically modeled using dimensionality reduction techniques, while concurrently, trajectory representations like splines and autoregressive models are widely used to exploit the temporal regularity of deformation. In this article, we present the bilinear spatiotemporal basis as a model that simultaneously exploits spatial and temporal regularity while maintaining the ability to generalize well to new sequences. This factorization allows the use of analytical, predefined functions to represent temporal variation (e.g., B-Splines or the Discrete Cosine Transform) resulting in efficient model representation and estimation. The model can be interpreted as representing the data as a linear combination of spatiotemporal sequences consisting of shape modes oscillating over time at key frequencies. We apply the bilinear model to natural spatiotemporal phenomena, including face, body, and cloth motion data, and compare it in terms of compaction, generalization ability, predictive precision, and efficiency to existing models. We demonstrate the application of the model to a number of graphics tasks including labeling, gap-filling, de-noising, and motion touch-up.  #Image <img src='http://graphics.cs.cmu.edu/projects/bilinearbasis/bilinear_files/facefire_axes.jpg' width='95%'>",
        "id": "siggraph10",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph21" : {
        "time": 21,
        "html": "<b> Three-dimensional Proxies for Hand-drawn Characters </b><br><br> #Authors: Eakta Jain, Yaser Sheikh, Moshe Mahler, Jessica Hodgins <br><br> #Abstract: Drawing shapes by hand and manipulating computer-generated objects are the two dominant forms of animation. Though each medium has its own advantages, the techniques developed for one medium are not easily leveraged in the other medium because hand animation is two-dimensional, and inferring the third dimension is mathematically ambiguous. A second challenge is that the character is a consistent three-dimensional (3D) object in computer animation while hand animators introduce geometric inconsistencies in the two-dimensional (2D) shapes to better convey a character's emotional state and personality. In this work, we identify 3D proxies to connect hand-drawn animation and 3D computer animation. We present an integrated approach to generate three levels of 3D proxies: single-points, polygonal shapes, and a full joint hierarchy. We demonstrate how this approach enables one medium to take advantage of techniques developed for the other; for example, 3D physical simulation is used to create clothes for a hand-animated character, and a traditionally trained animator is able to influence the performance of a 3D character while drawing with paper and pencil. #Image <img src='http://graphics.cs.cmu.edu/projects/threeDproxy/webpageteaser_3dproxy.png' width='95%'>",
        "id": "siggraph21",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph22" : {
        "time": 22,
        "html": "<b> How Do Humans Sketch Objects?  </b><br><br> #Authors: Mathias Eitz, James Hays and Marc Alexa <br><br> #Abstract: Humans have used sketching to depict our visual world since prehistoric times. Even today, sketching is possibly the only rendering technique readily available to all humans. This paper is the first large scale exploration of human sketches. We analyze the distribution of non-expert sketches of everyday objects such as 'teapot' or 'car'. We ask humans to sketch objects of a given category and gather 20,000 unique sketches evenly distributed over 250 object categories. With this dataset we perform a perceptual study and find that humans can correctly identify the object category of a sketch 73% of the time. We compare human performance against computational recognition methods. We develop a bag-of-features sketch representation and use multi-class support vector machines, trained on our sketch dataset, to classify sketches. The resulting recognition method is able to identify unknown sketches with 56% accuracy (chance is 0.4%). Based on the computational model, we demonstrate an interactive sketch recognition system. We release the complete crowd-sourced dataset of sketches to the community. #Image <img src='http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/teaser_siggraph.jpg' width='95%'>",
        "id": "siggraph22",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph23" : {
        "time": 23,
        "html": "<b> CrossShade: Shading Concept Sketches Using Cross-Section Curves </b><br><br> #Authors: Cloud Shao, Adrien Bousseau, Alla Sheffer, Karan Singh <br><br> #Abstract: CrossShade allows the creation of 3D-looking shaded production drawings from concept sketches. We use artist-drawn cross-section lines to automatically infer surface information across the sketch, enabling 3D-like rendering. Cross-sections function as an aid to both sketch creation and viewer understanding of the depicted 3D shape. In particular, intersections of these curves, or cross-hairs, convey valuable 3D information that viewers compose into a mental model of the sketch. We use this information to estimate the surface normals. #Image <img src='http://crossshade.com/images/attract2_thumb.png' width='95%'>",
        "id": "siggraph23",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph24" : {
        "time": 24,
        "html": "<b> Learning Hatching for Pen-and-Ink Illustration of Surfaces </b><br><br> #Authors: Evangelos Kalogerakis, Derek Nowrouzezahrai, Simon Breslav, Aaron Hertzmann <br><br> #Abstract: This paper presents an algorithm for learning hatching styles from line drawings. An artist draws a single hatching illustration of a 3D object. Their strokes are analyzed to extract the following per-pixel properties: hatching level (hatching, cross-hatching, or no strokes), stroke orientation, spacing, intensity, length, and thickness. A mapping is learned from input geometric, contextual and shading features of the 3D object to these hatching properties, using classification, regression, and clustering techniques. Then, a new illustration can be generated in the artist’s style, as follows. First, given a new view of a 3D object, the learned mapping is applied to synthesize target stroke properties for each pixel. A new illustration is then generated by synthesizing hatching strokes according to the target properties. #Image <img src='http://people.cs.umass.edu/~kalo/papers/MLHatching/ML_Hatching_Teaser.jpg' width='95%'>",
        "id": "siggraph24",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph25" : {
        "time": 25,
        "html": "<b> HelpingHand: Example-based Stroke Stylization </b><br><br> #Authors: Jingwan Lu, Fisher Yu, Adam Finkelstein, Stephen DiVerdi <br><br> #Abstract: Digital painters commonly use a tablet and stylus to drive software like Adobe Photoshop. A high quality stylus with 6 degrees of freedom (DOFs: 2D position, pressure, 2D tilt, and 1D rotation) coupled to a virtual brush simulation engine allows skilled users to produce expressive strokes in their own style. However, such devices are difficult for novices to control, and many people draw with less expensive (lower DOF) input devices. This paper presents a data-driven approach for synthesizing the 6D hand gesture data for users of low-quality input devices. Offline, we collect a library of strokes with 6D data created by trained artists. Online, given a query stroke as a series of 2D positions, we synthesize the 4D hand pose data at each sample based on samples from the library that locally match the query. This framework optionally can also modify the stroke trajectory to match characteristic shapes in the style of the library. Our algorithm outputs a 6D trajectory that can be fed into any virtual brush stroke engine to make expressive strokes for novices or users of limited hardware. #Image <img src='http://gfx.cs.princeton.edu/pubs/Lu_2012_HES/helpingHand.png' width='95%'>",
        "id": "siggraph25",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph26" : {
        "time": 26,
        "html": "<b> Stress Relief: Improving Structural Strength of 3D Printable Objects </b><br><br> #Authors: Stava, O., Vanek, J., Benes, B. , Carr, N., & Mech, R. <br><br> #Abstract: 3D printing is a rapidly maturing area that has shown great progress over the past couple of years. It is now possible to produce 3D printed objects with exceptionally high fidelity and precision. However, while the quality of 3D printing has gone up, both the time to print and material costs have remained high. Moreover, there is no guarantee that a printed model is structurally sound. Many times, the printed product does not survive cleaning, transportation, or handling, or it even collapses under its own weight. We present a system that addresses this issue by providing automatic detection and correction of the problematic cases. The structural problems are detected by combining a lightweight structure analysis solver with 3D medial axis approximations. After areas with high structural stress are found, the model is corrected by combining three approaches: hollowing, thickening, and strut insertion. This detection and correction repeats until all problematic cases are corrected. Our process is designed to create a model that is visually similar to the original model, while possessing greater structural integrity #Image <img src='http://hpcg.purdue.edu/img/publications/164_big.jpg' width='95%'>",
        "id": "siggraph26",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph27" : {
        "time": 27,
        "html": "<b> Fabricating Articulated Characters using Skinned Meshes </b><br><br> #Authors: Moritz Bächer, Bernd Bickel, Doug L. James, Hanspeter Pfister <br><br> #Abstract: Articulated deformable characters are widespread in computer animation. Unfortunately, we lack methods for their automatic fabrication using modern additive manufacturing (AM) technologies. We propose a method that takes a skinned mesh as input, then estimates a fabricatable single-material model that approximates the 3D kinematics of the corresponding virtual articulated character in a piecewise linear manner. We first extract a set of potential joint locations. From this set, together with optional, user-specified range constraints, we then estimate mechanical friction joints that satisfy inter-joint non-penetration and other fabrication constraints. To avoid brittle joint designs, we place joint centers on an approximate medial axis representation of the input geometry, and maximize each joint’s minimal cross-sectional area. We provide several demonstrations, manufactured as single, assembled pieces using 3D printers. #Image <img src='http://www.baecher.info/publications/fab_char_sig12_images/2_cropped_more_small.png' width='95%'>",
        "id": "siggraph27",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph28" : {
        "time": 28,
        "html": "<b> Beady: Interactive Beadwork Design and Construction </b><br><br> #Authors: Yuki Igarashi, Takeo Igarashi and Jun Mitani <br><br> #Abstract: We introduce the interactive system ``Beady'' to assist the design and construction of customized 3D beadwork. The user first creates a polygonal mesh model called the design model that represents the overall structure of the beadwork. Each edge of the mesh model corresponds to a bead in the beadwork. We provide two methods to create the design model. One is interactive modeling from scratch. The user defines the mesh topology with gestural interaction and the system continuously adjusts edge lengths by considering the physical constraints among neighboring beads. The other is automatic conversion that takes an existing polygonal model as input and generates a near-hexagonal mesh model with a near-uniform edge length as output. The system then converts the design model into a beadwork model with the appropriate wiring. Computation of an appropriate wiring path requires careful consideration, and we present an algorithm based on face stripification of the mesh. The system also provides a visual step-by-step guide to assist the manual beadwork construction process. We show several beadwork designs constructed by the authors and by test users using the system. #Image <img src='http://www.geocities.jp/igarashi_lab/beady/beady.jpg' width='95%'>",
        "id": "siggraph28",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph29" : {
        "time": 29,
        "html": "<b> Plastic Trees: Interactive Self-Adapting Botanical Tree Models </b><br><br> #Authors: Sören Pirk, Ondrej Stava, Julian Kratt, Michel Abdul-Massih, Boris Neubert, Radomír Měch, Bedrich Benes, Oliver Deussen <br><br> #Abstract: We present a dynamic tree modeling and representation technique that allows complex tree models to interact with their environment interactively. Our method uses changes in the light distribution and proximity to solid obstacles and other trees to impose biologically motivated transformations on a skeletal representation of the tree's main branches and its procedurally generated foliage. Parts of the tree are transformed only when required, thus our approach is much faster than common algorithms such as Open L-Systems or space colonization methods. Input is a skeleton-based tree geometry that can be computed from common tree production systems or from reconstructed laser scanning models. Our approach enables content creators to directly interact with trees and to create biologically convincing ecosystems interactively. We present different interaction types and evaluate our method by comparing our transformations to biologically based growth simulation techniques. #Image <img src='http://graphics.uni-konstanz.de/publikationen/2012/plastic_trees/website/images/teaser.jpg' width='95%'>",
        "id": "siggraph29",
        "parent": "none",
        "creator": "siggraph"
    },
    "siggraph30" : {
        "time": 30,
        "html": "<b>Stochastic Tomography and its Applications in 3D Imaging of Mixing Fluids </b><br><br> #Authors: James Gregson, Michael Krimerman, Matthias B. Hullin and Wolfgang Heidrich <br><br> #Abstract: We present a novel approach for highly detailed 3D imaging of turbulent fluid mixing behaviors. The method is based on visible light computed tomography, and is made possible by a new stochastic tomographic reconstruction algorithm based on random walks similar to Metropolis sampling. We show that this new stochastic algorithm is competitive with specialized tomography solvers such as SART, but can also easily include arbitrary convex regularizers that make it possible to obtain high-quality reconstructions with a very small number of views. Finally, we demonstrate that the same stochastic tomography approach can also be used to directly re-render arbitrary 2D projections without the need to ever store a 3D volume grid.  #Image <img src='http://www.cs.ubc.ca/labs/imager/tr/2012/StochasticTomography/teaser.png' width='95%'>",
        "id": "siggraph30",
        "parent": "none",
        "creator": "siggraph"
    }
}