tweets = {
    "siggraph0": {
        "time": 0, 
        "html": "<b> Sketch-Based Shape Retrieval</b><br><br> #Authors: Mathias Eitz, Ronald Richter, Tamy Boubekeur, Kristian Hildebrand and Marc Alexa<br><br> #Abstract: We develop a system for 3D object retrieval based on sketched feature lines as input. For objective evaluation, we collect a large number of query sketches from human users that are related to an existing data base of objects. The sketches turn out to be generally quite abstract with large local and global deviations from the original shape. Based on this observation, we decide to use a bag-of-features approach over computer generated line drawings of the objects. We develop a targeted feature transform based on Gabor filters for this system. We can show objectively that this transform is better suited than other approaches from the literature developed for similar tasks. Moreover, we demonstrate how to optimize the parameters of our, as well as other approaches, based on the gathered sketches. In the resulting comparison, our approach is significantly better than any other system described so far. #Image <img src='http://homes.cs.washington.edu/~felicia0/images/twitify/siggraph0.png' width='95%'>", 
        "id": "siggraph0", 
        "parent": "none", 
        "creator": "siggraph"
    }, 
    "siggraph1": {
        "time": 1, 
        "html": "<b>How Do Humans Sketch Objects?</b><br><br> #Authors: Mathias Eitz, James Hays and Marc Alexa<br><br> #Abstract: Humans have used sketching to depict our visual world since prehistoric times. Even today, sketching is possibly the only rendering technique readily available to all humans. This paper is the first large scale exploration of human sketches. We analyze the distribution of non-expert sketches of everyday objects such as 'teapot' or 'car'. We ask humans to sketch objects of a given category and gather 20,000 unique sketches evenly distributed over 250 object categories. With this dataset we perform a perceptual study and find that humans can correctly identify the object category of a sketch 73% of the time. We compare human performance against computational recognition methods. We develop a bag-of-features sketch representation and use multi-class support vector machines, trained on our sketch dataset, to classify sketches. The resulting recognition method is able to identify unknown sketches with 56% accuracy (chance is 0.4%). Based on the computational model, we demonstrate an interactive sketch recognition system. We release the complete crowd-sourced dataset of sketches to the community. #Image <img src='http://homes.cs.washington.edu/~felicia0/images/twitify/siggraph1.png' width='95%'>", 
        "id": "siggraph1", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph2": {
        "time": 2, 
        "html": "<b>Tools for Placing Cuts and Transitions in Interview Video</b><br><br> #Authors: Floraine Berthouzoz, Wilmot Li, Maneesh Agrawala<br><br> #Abstract: We present a set of tools designed to help editors place cuts and create transitions in interview video. To help place cuts, our interface links a text transcript of the video to the corresponding locations in the raw footage. It also visualizes the suitability of cut locations by analyzing the audio/visual features of the raw footage to find frames where the speaker is relatively quiet and still. With these tools editors can directly highlight segments of text, check if the endpoints are suitable cut locations and if so, simply delete the text to make the edit. For each cut our system generates visible (e.g. jump-cut, fade, etc.) and seamless, hidden transitions.We present a hierarchical, graph-based algorithm for efficiently generating hidden transitions that considers visual features specific to interview footage. We also describe a new data-driven technique for setting the timing of the hidden transition. Finally, our tools offer a one click method for seamlessly removing ?ums? and repeated words as well as inserting natural-looking pauses to emphasize semantic content. We apply our tools to edit a variety of interviews and also show how they can be used to quickly compose multiple takes of an actor narrating a story. #Image <img src='http://homes.cs.washington.edu/~felicia0/images/twitify/siggraph2.png' width='95%'>", 
        "id": "siggraph2", 
        "parent": "none", 
        "creator": "siggraph"
    },
     "siggraph3": {
        "time": 3, 
        "html": "<b>Revel: Programming the Sense of Touch</b><br><br> #Authors: Olivier Bau, ivan poupyrev<br><br> #Abstract: REVEL is a new wearable tactile technology that modifies the user?s tactile perception of the physical world. Current tactile technologies enhance objects and devices with various actuators to create rich tactile sensations, limiting the experience to the interaction with instrumented devices. In contrast, REVEL can add artificial tactile sensations to almost any surface or object, with very little if any instrumentation of the environment. As a result, REVEL can provide dynamic tactile sensations on touch screens as well as everyday objects and surfaces in the environment, such as furniture, walls, wooden and plastic objects, and even human skin. REVEL is based on Reverse Electrovibration. It injects a weak electrical signal into anywhere on the user?s body, creating an oscillating electrical field around the user?s skin. When sliding his or her fingers on a surface of the object, the user perceives highly distinctive tactile textures that augment the physical object. Varying the properties of the signal, such as the shape, amplitude and frequency, can provide a wide range of tactile sensations. #Image <img src='http://homes.cs.washington.edu/~felicia0/images/twitify/siggraph3.png' width='95%'>", 
        "id": "siggraph3", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph4": {
        "time": 4, 
        "html": "<b>What Makes Paris Look like Paris?</b><br><br> #Authors: Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei A. Efros<br><br> #Abstract: Given a large repository of geotagged imagery, we seek to automatically find visual elements, e.g. windows, balconies, and street signs, that are most distinctive for a certain geo-spatial area, for example the city of Paris. This is a tremendously difficult task as the visual features distinguishing architectural elements of different places can be very subtle. In addition, we face a hard search problem: given all possible patches in all images, which of them are both frequently occurring and geographically informative? To address these issues, we propose to use a discriminative clustering approach able to take into account the weak geographic supervision. We show that geographically representative image elements can be discovered automatically from Google Street View imagery in a discriminative manner. We demonstrate that these elements are visually interpretable and perceptually geo-informative. The discovered visual elements can also support a variety of computational geography tasks, such as mapping architectural correspondences and influences within and across cities, finding representative elements at different geo-spatial scales, and geographically-informed image retrieval. #Image <img src='http://homes.cs.washington.edu/~felicia0/images/twitify/siggraph4.png' width='95%'>", 
        "id": "siggraph4", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph5": {
        "time": 5, 
        "html": "<b>HelpingHand: Example-based Stroke Stylization</b><br><br> #Authors: Jingwan Lu, Fisher Yu, Adam Finkelstein, Stephen DiVerdi<br><br> #Abstract: Digital painters commonly use a tablet and stylus to drive software like Adobe Photoshop. A high quality stylus with 6 degrees of freedom (DOFs: 2D position, pressure, 2D tilt, and 1D rotation) coupled to a virtual brush simulation engine allows skilled users to produce expressive strokes in their own style. However, such devices are difficult for novices to control, and many people draw with less expensive (lower DOF) input devices. This paper presents a data-driven approach for synthesizing the 6D hand gesture data for users of low-quality input devices. Offline, we collect a library of strokes with 6D data created by trained artists. Online, given a query stroke as a series of 2D positions, we synthesize the 4D hand pose data at each sample based on samples from the library that locally match the query. This framework optionally can also modify the stroke trajectory to match characteristic shapes in the style of the library. Our algorithm outputs a 6D trajectory that can be fed into any virtual brush stroke engine to make expressive strokes for novices or users of limited hardware. #Image <img src='http://homes.cs.washington.edu/~felicia0/images/twitify/siggraph5.png' width='95%'>", 
        "id": "siggraph5", 
        "parent": "none", 
        "creator": "siggraph"
    }
    ,
    "siggraph6": {
        "time": 6, 
        "html": "<b>EXPLORING COLLECTIONS OF 3D MODELS USING FUZZY CORRESPONDENCES</b><br><br> #Authors: Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen DiVerdi, Thomas Funkhouser <br><br> #Abstract: Large collections of 3D models from the same object class (e.g., chairs, cars, animals) are now commonly available via many public repositories, but exploring the range of shape variations across such collections remains a challenging task. In this work, we present a new exploration interface that allows users to browse collections based on similarities and differences between shapes in user-specified regions of interest (ROIs). To support this interactive system, we introduce a novel analysis method for computing similarity relationships between points on 3D shapes across a collection. We encode the inherent ambiguity in these relationships using fuzzy point correspondences and propose a robust and efficient computational framework that estimates fuzzy correspondences using only a sparse set of pairwise model alignments. We evaluate our analysis method on a range of correspondence benchmarks and report sub- stantial improvements in both speed and accuracy over existing alternatives. In addition, we demonstrate how fuzzy correspondences enable key features in our exploration tool, such as automated view alignment, ROI-based similarity search, and faceted browsing. #Image <img src='http://homes.cs.washington.edu/~felicia0/images/twitify/siggraph6.png' width='95%'>", 
        "id": "siggraph6", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph20": {
        "time": 20, 
"html": "<b>EXPLORING COLLECTIONS OF 3D MODELS USING FUZZY CORRESPONDENCES</b><br><br> #Authors: Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen DiVerdi, Thomas Funkhouser <br><br> #Introduction #Paragraph1: 								Increasing availability of powerful modeling software and 3D acquisition devices has led to rapidly growing repositories of 3D models (e.g., TurboSquid, Google 3D Warehouse, etc.). Yet, the task of exploring such large 3D repositories remains an important and challenging problem. In particular, while most online databases make it easy for users to select sets of similar models (which we refer to as model collections) using text-based ?ltering, understanding the range of variations within such collections is typically much more dif?cult (see also [Ovsjanikov et al. 2011]).", 
        "id": "siggraph20", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph19": {
        "time": 19, 
"html": "<b>EXPLORING COLLECTIONS OF 3D MODELS USING FUZZY CORRESPONDENCES</b><br><br> #Authors: Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen DiVerdi, Thomas Funkhouser <br><br> #Introduction #Paragraph2: For many object classes, one key challenge is that the shape can vary in many different ways, and users may be interested in exploring different types of variations. For example, within a collection of chair models, one user may want to see how the backs of chairs attach to the seat (smoothly merge, right angle, etc.) while another user may only want to see chairs whose back legs are at a certain angle (see Figure 1). Even within a single exploration session, a user may want to de?ne the exploration space using multiple different attributes (e.g., chairs with a stem base and curved back). Given the spectrum of different possible exploration criteria, a static prede?ned organization of the data is clearly not suf?cient. This is especially true for very large and diverse collections, like the ones we ?nd for many object classes in the Google 3D Warehouse (e.g., chairs, cars, animals).", 
        "id": "siggraph19", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph18": {
        "time": 18, 
"html": "<b>EXPLORING COLLECTIONS OF 3D MODELS USING FUZZY CORRESPONDENCES</b><br><br> #Authors: Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen DiVerdi, Thomas Funkhouser <br><br> #Introduction #Paragraph3: In this work, we present a new analysis tool and exploration interface for 3D model collections. As a key feature, we allow users to directly specify regions of interest (ROI) on example shapes in order to guide subsequent exploration actions. Thus we can support the browsing scenarios described above; the user selects the appropriate ROIs on one or more chairs, and the system automatically organizes the rest of the chairs based on their similarity to the speci?ed region. From a Human Computer Information Retrieval perspective, our system can be described as a type offaceted browsing interface, which has proven to be a very effective and popular method for exploring diverse collections of items with a wide variety of attributes [Hearst 2006]. For example, faceted interfaces are very common for online shopping websites where attributes like price, date of release, popularity, etc. act as prede?ned facets for navigation. In contrast, our facets are de?ned interactively as the user selects speci?c ROIs (see supplementary video).", 
        "id": "siggraph18", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph17": {
        "time": 17, 
"html": "<b>EXPLORING COLLECTIONS OF 3D MODELS USING FUZZY CORRESPONDENCES</b><br><br> #Authors: Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen DiVerdi, Thomas Funkhouser <br><br> #Introduction #Paragraph4: The main technical challenge in realizing our proposed interface is how to relate any arbitrary user selected region on one shape to all the other models in the collection. Since user-selected ROIs do not necessarily match pre-segmented parts, consistent part-level segmentation is not suf?cient. Rather, we prefer correspondences for individual points. Yet, automatically establishing one-to-one point correspondences across a model collection is dif?cult and often ambiguous, especially when the collection includes diverse shapes. For example, how do the chair bases in the the top row of Figure 1 correspond to the chair legs in the bottom row?", 
        "id": "siggraph17", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph16": {
        "time": 16, 
"html": "<b>EXPLORING COLLECTIONS OF 3D MODELS USING FUZZY CORRESPONDENCES</b><br><br> #Authors: Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen DiVerdi, Thomas Funkhouser <br><br> #Introduction #Paragraph5: To address this issue, we encode the ambiguity of correspondences in model collections with fuzzy correspondences1 . Speci?cally, given a collection of N shapes S := fS1;S2; : : : ;SNg, we use fuzzy correspondences as a function f(pi ; pj) : S  S ! R to denote a continuous similarity measure between points pi 2 Sm and pj 2 Sn.", 
        "id": "siggraph16", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph15": {
        "time": 15, 
"html": "<b>EXPLORING COLLECTIONS OF 3D MODELS USING FUZZY CORRESPONDENCES</b><br><br> #Authors: Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen DiVerdi, Thomas Funkhouser <br><br> #Introduction #Paragraph6: To estimate fuzzy correspondences, f(pi ; pj), we utilize geometric matching methods that align pairs of shapes. Although these methods are computationally expensive and often produce noisy alignments, we observe that for collections of shapes from the same class a correspondence matrix that stores high values for corresponding pairs of points is (i) sparse, (ii) low-rank, and (iii) its rank does not depend on the number of models. We propose a method based on diffusion maps [Nadler et al. 2006] to reconstruct f from sparse and noisy samples (i.e., pairwise alignments) and an iterative procedure to re?ne f by adaptively sampling based on the current estimate.", 
        "id": "siggraph15", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph14": {
        "time": 14, 
"html": "<b>EXPLORING COLLECTIONS OF 3D MODELS USING FUZZY CORRESPONDENCES</b><br><br> #Authors: Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen DiVerdi, Thomas Funkhouser <br><br> #Introduction #Paragraph7: We test the accuracy of our estimate of f using the correspondence benchmark for intrinsically-similar shapes [Kim et al. 2011]. We also introduce a new correspondence benchmark of 111 chairs and 86 commercial airplanes using data obtained from the Google 3D Warehouse. Our method successfully utilizes the collection to improve alignments of shapes in comparison to existing methods (see Figures 11, 12, and supplementary material). ", 
        "id": "siggraph14", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph13": {
        "time": 13, 
"html": "<b>EXPLORING COLLECTIONS OF 3D MODELS USING FUZZY CORRESPONDENCES</b><br><br> #Authors: Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen DiVerdi, Thomas Funkhouser <br><br> #Introduction #Paragraph8: Contributions. In summary, we  introduce an approach for using fuzzy correspondences to understand similarity relations across 3D model collections,  propose a robust and ef?cient algorithm to compute fuzzy correspondences from sparse and noisy pairwise alignments,  evaluate our algorithm on correspondence benchmarks and report substantial improvement over existing alternatives, and  present an interactive exploration tool for large model collections that uses fuzzy correspondences to support view alignment, ROI-based similarity search, and faceted exploration. ", 
        "id": "siggraph13", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph27": {
        "time": 27, 
"html": "<b>HelpingHand: Example-based Stroke Stylization</b><br><br> #Authors: Jingwan Lu, Fisher Yu, Adam Finkelstein, Stephen DiVerdi<br><br> #Introduction #Paragraph1: Artists wielding traditional drawing instruments such as a pencil, a stick of charcoal or a paint brush exercise fine control over the interplay between their gestures and the physical medium. The appearance of the strokes is governed not only by the path of the instrument but also the pressure and angle (see inset). Digital artists often work with high quality tablets that track a stylus with 6 DOFs (2D position, pressure, 2D tilt, and rotation), which painting software can use to simulate the interaction of digital brushes in various media with increasing fidelity [Baxter and Lin 2004]. #Image <img src='http://homes.cs.washington.edu/~felicia0/images/twitify/siggraph12.png' width='415px'>", 
        "id": "siggraph27", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph26": {
        "time": 26, 
"html": "<b>HelpingHand: Example-based Stroke Stylization</b><br><br> #Authors: Jingwan Lu, Fisher Yu, Adam Finkelstein, Stephen DiVerdi<br><br> #Introduction #Paragraph2: However, most people drawing on digital devices today do not use 6-DOF tablets, for two reasons. First, high-quality tablets are relatively expensive as compared with other input devices like mice (2-DOF), multitouch screens (2-DOF), or mass-market tablets (3- or 5-DOF). Second, even if everyone owned a 6-DOF tablet, most people would not have the training and experience to be able to wield it effectively. To achieve high quality calligraphy, for example, takes many hours (perhaps years) of practice.>", 
        "id": "siggraph26", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph25": {
        "time": 25, 
"html": "<b>HelpingHand: Example-based Stroke Stylization</b><br><br> #Authors: Jingwan Lu, Fisher Yu, Adam Finkelstein, Stephen DiVerdi<br><br> #Introduction #Paragraph3: This paper describes a method whereby a non-expert draws a 2D query stroke (Figure 1a), perhaps using an inexpensive input device, and missing DOFs are synthesized based on a library of examples supplied by an artist (Figure 2a). The resulting marks follow the trajectory drawn by the non-expert but convey the gestural style of the artist (Figure 1b). We also show how the approach can be extended to approximate the input trajectory while adopting some of the character of the strokes in the artist?s library (Figure 1c). This option is beneficial for users who are not confident in their own style, or when using a coarse input device like a mouse. One can easily change style by choosing a different library (Figure 1d).>", 
        "id": "siggraph25", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph24": {
        "time": 24, 
"html": "<b>HelpingHand: Example-based Stroke Stylization</b><br><br> #Authors: Jingwan Lu, Fisher Yu, Adam Finkelstein, Stephen DiVerdi<br><br> #Introduction #Paragraph4: To synthesize hand pose, we address this problem: given a library of 6-DOF strokes (drawn by the artist) and a 2-DOF query stroke (drawn by the user), produce 4-DOFs to accompany the query so that the 6-DOF (2+4) combination looks like the strokes in the library. Trajectory stylization optionally replaces the 2-DOF trajectory as well. There is no single correct answer because the same path can be drawn differently, even by a single artist, so our goal is to synthesize plausible gestures.>", 
        "id": "siggraph24", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph23": {
        "time": 23, 
"html": "<b>HelpingHand: Example-based Stroke Stylization</b><br><br> #Authors: Jingwan Lu, Fisher Yu, Adam Finkelstein, Stephen DiVerdi<br><br> #Introduction #Paragraph5: Our method produces gestures that are plausibly in the style of the artist?s library. It performs at interactive rates to provide immediate feedback to the user during drawing and to make it easy to choose amongst different styles. Finally, while we have designed the system for interactive use by non-experts or people without highquality tablets, the same method can apply stylization to any source of line art, including the output of vector illustration software or computer-generated line drawings based on 3D models.>", 
        "id": "siggraph23", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph22": {
        "time": 22, 
"html": "<b>HelpingHand: Example-based Stroke Stylization</b><br><br> #Authors: Jingwan Lu, Fisher Yu, Adam Finkelstein, Stephen DiVerdi<br><br> #Introduction #Paragraph6: The main contributions of this paper include stroke stylization via example-based synthesis of hand pose?data needed to control a virtual brush?as well as optional trajectory modification. Through data collection and analysis we offer understanding of how hand pose relates to individual writing style. Finally, we explore the space of candidate algorithms and evaluate them quantitatively and qualitatively, concluding with a demonstration of practical results.>", 
        "id": "siggraph22", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph31": {
        "time": 31, 
"html": "<b>Sketch-Based Shape Retrieval</b><br><br> #Authors: Mathias Eitz, Ronald Richter, Tamy Boubekeur, Kristian Hildebrand and Marc Alexa<br><br> #Introduction #Paragraph1: Working with large collections of 3D models requires fast contentbased retrieval techniques, especially since public collections are often insufficiently annotated. In that case a keyword based search alone is not promising. While research on example-based retrieval ? where users provide a full model as the query ? has recently found a lot of interest in the community [Tangelder and Veltkamp 2008], its practical application is dif?cult, since a good example is often not at hand. Instead, sketch-based retrieval has been proposed [Lof?er ¨2000; Funkhouser et al. 2003; Chen et al. 2003; Yoon et al. 2010;Shao et al. 2011], where users sketch the desired model as seenfrom one or more viewpoints. We consider sketch-based retrieval to be even more challenging than example-based retrieval as the querycontains only partial information about the projection of the shape.Most humans have limited drawing skills and lines may additionally deviate significantly from that projection. These properties of the input directly translate into desiderata of sketch-based shape retrieval systems: a) partial matching of feature lines of the shape in b) all potential viewing directions to the sketch, tolerating c) global and local deformation; and, clearly, the retrieval performance has to d) scale to large collections.>", 
        "id": "siggraph31", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph30": {
        "time": 30, 
"html": "<b>Sketch-Based Shape Retrieval</b><br><br> #Authors: Mathias Eitz, Ronald Richter, Tamy Boubekeur, Kristian Hildebrand and Marc Alexa<br><br> #Introduction #Paragraph2: We present, to our knowledge, the ?rst approach that addresses all of the desiderata. The approach is based on the visual analysis of meshes: we sample the set of likely view directions, generate line drawings with state of the art line rendering techniques, and encode the line drawings with a bag-of-features approach. This choice is directly related to the requirements. First, rather than trying to match projected lines to shape features in 3D, we exploit current line art rendering techniques. They have reached a mature state, in which almost all lines drawn by humans are also generated by algorithms [Cole et al. 2008]. Second, bag-of-features approaches, which are well known in the image retrieval community [Sivic and Zisserman 2003], use local image descriptors that are independent of location. This is ideal, as it immediately enables partial matching and is resilient to global deformations. We achieve additional resilience to local deformations by quantization of the local image descriptors (identifying so-called ?visual words?) and matching based on histograms. This data reduction leads, third, to the desired fast query times.>", 
        "id": "siggraph30", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph29": {
        "time": 29, 
"html": "<b>Sketch-Based Shape Retrieval</b><br><br> #Authors: Mathias Eitz, Ronald Richter, Tamy Boubekeur, Kristian Hildebrand and Marc Alexa<br><br> #Introduction #Paragraph3: We make use of successful techniques from other domains where appropriate and provide the following novel contributions: A large-scale benchmark for sketch-based retrieval systems. The benchmark is based on a real-world dataset of 1,914 sketches gathered from a large variety of participants in a perceptual experiment. We provide this dataset as a free resource. A new feature transform based on a bank of Gabor ?lters that is tuned to the requirements of sketch-based shape retrieval. This descriptor outperforms other existing transformations. A general approach to determine optimal parameters for such feature transformations. We demonstrate that even existing systems can be improved using this approach. >", 
        "id": "siggraph29", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph28": {
        "time": 28, 
"html": "<b>Sketch-Based Shape Retrieval</b><br><br> #Authors: Mathias Eitz, Ronald Richter, Tamy Boubekeur, Kristian Hildebrand and Marc Alexa<br><br> #Introduction #Paragraph4: Overall, this leads to a system with high quality retrieval performance as we demonstrate in our objective evaluation as well as the accompanying video using a large variety of real-world user sketches. We also demonstrate the power of our system in Fig. 1 where we gather all objects for a complete scene in about two minutes. However, we also ?nd that the real-world dataset of sketches gathered in the experiment is challenging for current systems. In particular, our dataset reveals that allowing only closed contour curves for retrieval [Chen et al. 2003] oversimpli?es reality: a large majority of our participants? sketches contain a substantial amount of interior lines. The insights gained from an analysis of our dataset open up several promising areas of further research which we identify in Sec. 8.>", 
        "id": "siggraph28", 
        "parent": "none", 
        "creator": "siggraph"
    }, 
    "siggraph35": {
        "time": 35, 
        "html": "<b>How Do Humans Sketch Objects?</b><br><br> #Authors: Mathias Eitz, James Hays and Marc Alexa<br><br> #Introduction #Paragraph1: Sketching is a universal form of communication. Since prehistoric times, people have rendered the visual world in sketch-like petroglyphs or cave paintings. Such pictographs predate the appearance of language by tens of thousands of years and today the ability to draw and recognize sketched objects is ubiquitous. In fact, recent neuroscience work suggests that simple, abstracted sketches activate our brain in similar ways to real stimuli [Walther et al. 2011]. Despite decades of graphics research, sketching is the only mechanism for most people to render visual content. However, there has never been a formal study of how people sketch objects and how well such sketches can be recognized by humans and computers. We examine these topics for the ?rst time and demonstrate applications of computational sketch understanding. In this paper we use the term ?sketch? to mean a non-expert, abstract pictograph and not to imply any particular medium (e.g. pencil and paper). ", 
        "id": "siggraph35", 
        "parent": "none", 
        "creator": "siggraph"
    }, 
    "siggraph34": {
        "time": 34, 
        "html": "<b>How Do Humans Sketch Objects?</b><br><br> #Authors: Mathias Eitz, James Hays and Marc Alexa<br><br> #Introduction #Paragraph2: There exists signi?cant prior research on retrieving images or 3d models based on sketches. The assumption in all of these works is that, in some well-engineered feature space, sketched objects resemble their real-world counterparts. But this fundamental assumption is often violated ? most humans are not faithful artists. Instead people use shared, iconic representations of objects (e.g. stick ?gures) or they make dramatic simpli?cations or exaggerations (e.g. pronounced ears on rabbits). Thus to understand and recognize sketches an algorithm must learn from a training dataset of real sketches, not photos or 3d models. Because people represent the same object using differing degrees of realism and distinct drawing styles (see Fig. 1), we need a large dataset of sketches which adequately samples these variations.", 
        "id": "siggraph34", 
        "parent": "none", 
        "creator": "siggraph"
    }, 
    "siggraph33": {
        "time": 33, 
        "html": "<b>How Do Humans Sketch Objects?</b><br><br> #Authors: Mathias Eitz, James Hays and Marc Alexa<br><br> #Introduction #Paragraph3: There also exists prior research in sketch recognition which tries to identify prede?ned glyphs in narrow domains (e.g. wire diagrams, musical scores). We instead identify objects such as snowmen, ice cream cones, giraffes, etc. This task is hard, because an average human is, unfortunately, not a faithful artist. Although both shape and proportions of a sketched object may be far from that of the corresponding real object, and at the same time sketches are an impoverished visual representation, humans are amazingly accurate at interpreting such sketches.", 
        "id": "siggraph33", 
        "parent": "none", 
        "creator": "siggraph"
    }, 
    "siggraph32": {
        "time": 32, 
        "html": "<b>How Do Humans Sketch Objects?</b><br><br> #Authors: Mathias Eitz, James Hays and Marc Alexa<br><br> #Introduction #Paragraph4: We ?rst de?ne a taxonomy of 250 object categories and acquire a large dataset of human sketches for the categories using crowdsourcing (Sec. 3). Based on the dataset we estimate how humans perform in recognizing the categories for each sketch (Sec. 4). We design a robust visual feature descriptor for sketches (Sec. 5). This feature permits not only unsupervised analysis of the dataset (Sec. 6), but also the computational recognition of sketches (Sec. 7). While we achieve a high computational recognition accuracy of 56% (chance is 0.4%), our study also reveals that humans still perform signi?cantly better than computers at this task. We show several interesting applications of the computational model (Sec. 8): apart from the interactive recognition of sketches itself, we also demonstrate that recognizing the category of a sketch could improve image retrieval. We hope that the use of sketching as a visual input modality opens up computing technology to a signi?cantly larger user base than text input alone. This paper is a ?rst steptoward this goal and we release the dataset to encourage future research in this domain.", 
        "id": "siggraph32", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph44": {
        "time": 44, 
        "html": "<b>Tools for Placing Cuts and Transitions in Interview Video</b><br><br> #Authors: Floraine Berthouzoz, Wilmot Li, Maneesh Agrawala<br><br> #Introduction #Paragraph1: Interview video frequently appears in broadcast news, documentary films, and online lectures. Such video usually focuses on the head and upper body of a person who explains a concept while looking at the camera. The challenge in editing interview video is to convert hours of raw footage into a concise clip that conveys the desired story. The process involves two steps: (1) choosing where to cut segments of the raw footage and (2) creating audio/visual transitions that connect the remaining segments into the ?nal clip. ", 
        "id": "siggraph44", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph43": {
        "time": 43, 
        "html": "<b>Tools for Placing Cuts and Transitions in Interview Video</b><br><br> #Authors: Floraine Berthouzoz, Wilmot Li, Maneesh Agrawala<br><br> #Introduction #Paragraph2: To place a cut, the editor must consider both the semantic content and the audio/visual attributes of the raw footage. Editors often begin by watching the entire raw footage several times and manually building an index that describes its content. Once the editor decides on the story, he can use the index to ?nd the segments corresponding to speci?c content. The editor must also account for the audio/visual attributes of the segments when deciding where to cut. For example, cutting while the speaker is in the middle of saying a word or is gesturing energetically is often undesirable because such cuts interrupt the audio/visual flow of the video [O?Steen 2009].", 
        "id": "siggraph43", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph42": {
        "time": 42, 
        "html": "<b>Tools for Placing Cuts and Transitions in Interview Video</b><br><br> #Authors: Floraine Berthouzoz, Wilmot Li, Maneesh Agrawala<br><br> #Introduction #Paragraph3: After choosing the rough position of a cut the editor must create a transition that connects the video segments before and after the cut. Such transitions fall into two main categories; Hidden Transitions conceal the cut by smoothly joining the segments. Thus, they allow viewers to remain focused on the continuous ?ow of semantic content in the video. Visible Transitionsintroduce a noticeable change in audio/visual content (e.g. jump-cut, fade, etc.) between the segments. They usually indicate a break in the ?ow of semantic content. ", 
        "id": "siggraph42", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph41": {
        "time": 41, 
        "html": "<b>Tools for Placing Cuts and Transitions in Interview Video</b><br><br> #Authors: Floraine Berthouzoz, Wilmot Li, Maneesh Agrawala<br><br> #Introduction #Paragraph4: Producing hidden transitions can be dif?cult as it requires either synthetically interpolating new frames or ?nding existing frames within the footage to seamlessly join the segments. Yet, interpolation [Mahajan et al. 2009; Brox et al. 2004] is only effective when the endpoint frames are similar in appearance, and manually searching for existing frames in the entire raw footage is impractical. As a result, editors often end up using visible transitions to join segments even when there is no break in the ?ow of semantic content. ", 
        "id": "siggraph41", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph40": {
        "time": 40, 
        "html": "<b>Tools for Placing Cuts and Transitions in Interview Video</b><br><br> #Authors: Floraine Berthouzoz, Wilmot Li, Maneesh Agrawala<br><br> #Introduction #Paragraph5: In this work, we present tools for interactively placing cuts and creating hidden as well as visible transitions in interview video footage. To help editors place the cuts, we combine crowdsourcing (castingwords.com) to obtain a text transcript of the raw footage, with text-audio alignment software [Virage ] to build an index that links each word in the transcript with the corresponding location in the footage. The editor can then directly edit the text script and our tools propagate the changes to the video. We also visualize the suitability of cut locations by analyzing the audio/visual features of the raw footage to ?nd frames where the speaker is relatively quiet and still (Figure 1). Together these tools allow the editor to focus on the semantic content and quickly select cut locations that do not interrupt the ?ow of the video in an unnatural manner. ", 
        "id": "siggraph40", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph39": {
        "time": 39, 
        "html": "<b>Tools for Placing Cuts and Transitions in Interview Video</b><br><br> #Authors: Floraine Berthouzoz, Wilmot Li, Maneesh Agrawala<br><br> #Introduction #Paragraph6: For each cut, our system automatically generates hidden and visible transitions so that the editor can see each possibility and choose whichever one is most appropriate. We present a new algorithm for efficiently generating hidden transitions. Our method builds on the general approach of constructing a similiarity graph between (b) Removing a phrase with low suitability (c) Removing ums and repeated words the raw frames and walking the graph to select the smoothest set of in-between frames [Schodl et al. 2000; Kwatra et al. 2003; ¨ Kemelmacher-Shlizerman et al. 2011]. Unique to our approach is a frame matching distance that considers facial appearance, body appearance, and location of the speaker within the frame. We also use hierarchical clustering to signi?cantly reduce the number of frameto-frame comparisons necessary to achieve good performance. ", 
        "id": "siggraph39", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph38": {
        "time": 38, 
        "html": "<b>Tools for Placing Cuts and Transitions in Interview Video</b><br><br> #Authors: Floraine Berthouzoz, Wilmot Li, Maneesh Agrawala<br><br> #Introduction #Paragraph7: To further refine the resulting transition we apply optical ?ow interpolation [Brox et al. 2004] with a new data-driven technique for selecting the number of interpolated frames. Our technique sets the timing of the transitions so that any motions of the speaker appear natural. Such timing is crucial for maintaining the ?ow of the video. ", 
        "id": "siggraph38", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph37": {
        "time": 37, 
        "html": "<b>Tools for Placing Cuts and Transitions in Interview Video</b><br><br> #Authors: Floraine Berthouzoz, Wilmot Li, Maneesh Agrawala<br><br> #Introduction #Paragraph8: Beyond placing simple cuts and transitions, our interface offers additional tools for increasing the quality of the ?nal clip. The transcript view highlights ?ums? and repeated words and gives editors a one click option for seamlessly removing them. Editors can also insert natural-looking pauses, in which the speaker appears to be at rest, to further emphasize important semantic content. ", 
        "id": "siggraph37", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph36": {
        "time": 36, 
        "html": "<b>Tools for Placing Cuts and Transitions in Interview Video</b><br><br> #Authors: Floraine Berthouzoz, Wilmot Li, Maneesh Agrawala<br><br> #Introduction #Paragraph9: We apply our tools on videos from a variety of sources including a corporate marketing department and professional journalists. In addition to interview editing, we show how our interface can be used to quickly compose multiple takes of an actor narrating a story. Finally, we demonstrate the success of our approach by providing visual comparisons to a number of other techniques.", 
        "id": "siggraph36", 
        "parent": "none", 
        "creator": "siggraph"
    },
     "siggraph49": {
        "time": 49, 
"html": "<b>Revel: Tactile Feedback Technology for Augmented Reality</b><br><br> #Authors: Olivier Bau, ivan poupyrev<br><br> #Introduction #Paragraph1: Augmented Reality has recently emerged as one of the key application areas of interactive computer graphics and is rapidly expanding from research laboratories into everyday use. The fundamental premise of AR is to enable us to interact with virtual objects immediately and directly, seeing, feeling and manipulating them just as we do with physical objects. Most AR applications, however, provide only visual augmentation of the real world and do not provide the means to let the user feel tactile, physical properties of virtual objects or to enhance the physical world with computer-generated tactile textures. The absence of tactile feedback does not allow us to take advantage of the powerful mechanisms of the human sense of touch and diminishes the quality of the experience.", 
        "id": "siggraph49", 
        "parent": "none", 
        "creator": "siggraph"
    },
     "siggraph48": {
        "time": 48, 
"html": "<b>Revel: Tactile Feedback Technology for Augmented Reality</b><br><br> #Authors: Olivier Bau, ivan poupyrev<br><br> #Introduction #Paragraph2: REVEL is an augmented reality (AR) tactile technology that allows us to change the tactile feeling of real world objects by augmenting them with virtual tactile textures(Figure 1). It is based on the principle of reverse electrovibration where a weak electrical signal is injected anywhere on the user?s body, creating an oscillating electrical field around the user?s fingers. As the user then slides his or her fingers on the surface of an object, the user perceives highly distinctive tactile textures overlying the physical object. By tracking touch locations, tactile textures can be dynamically modified in real time and enhanced with visual augmentation if required. The user?s hands remain free and unencumbered, so that users can continue their natural interaction with the world around them, unconstrained by tactile feedback technology. In a broad sense we are programmatically controlling the user?stactile perception. ", 
        "id": "siggraph48", 
        "parent": "none", 
        "creator": "siggraph"
    },
     "siggraph47": {
        "time": 47, 
"html": "<b>Revel: Tactile Feedback Technology for Augmented Reality</b><br><br> #Authors: Olivier Bau, ivan poupyrev<br><br> #Introduction #Paragraph3: REVEL advances our previous research on electrovibration-based tactile displays for touch surfaces, i.e., TeslaTouch [Bau, Poupyrev, et al. 2010]. In TeslaTouch the electrical signal was injected into the surface electrode of the touch screens, the classic technique to design tactile displays based on electrovibration, e.g., devices for the blind proposed in the early 70s [Strong, 1970]. In all these displays, including TeslaTouch, the tactile sensation is localized within a specific device augmented with tactile feedback, which is not scalable. Indeed, to add virtual tactile sensations to more objects or devices, all of them must be instrumented with tactile feedback apparatus. REVEL produces the same tactile effect, but reverses this dependency on individual object instrumentation. It instead injects tactile signals directly into the user?s body, so that the user becomes the carrier of the tactile signal at all times. The world and objects remain passive, requiring no instrumentation with additional technology. Therefore, this technology potentially allows for the creation of truly ubiquitous tactile interfaces that can be used anywhere and anytime. ", 
        "id": "siggraph47", 
        "parent": "none", 
        "creator": "siggraph"
    },
     "siggraph46": {
        "time": 46, 
"html": "<b>Revel: Tactile Feedback Technology for Augmented Reality</b><br><br> #Authors: Olivier Bau, ivan poupyrev<br><br> #Introduction #Paragraph4: The development of reverse electrovibration-based tactile displays requires an in-depth understanding of the basic physical principles of electrovibration. It involves the investigation of the interaction of electrostatic fields with human body, environment and various materials, as well as the exploration of the effect of their various properties on tactile sensations. Developing this body of knowledge and techniques that allow designing tactile feedback ?in the world? is one of the important contributions of this paper that advances the state of the art in electrovibration-based tactile interfaces. ", 
        "id": "siggraph46", 
        "parent": "none", 
        "creator": "siggraph"
    },
     "siggraph45": {
        "time": 45, 
"html": "<b>Revel: Tactile Feedback Technology for Augmented Reality</b><br><br> #Authors: Olivier Bau, ivan poupyrev<br><br> #Introduction #Paragraph5: In designing REVEL we were also motivated by a broader vision of tactile displays that presents tactile stimuli by altering user tactile perception [Kruijff et al. 2006]. Such tactile displays do not require environmental instrumentation and could be used anywhere, leading to the design of a truly ubiquitous tactile augmented reality as envisioned by early AR pioneers [Azuma 1997]. We refer to this new class of tactile devices as intrinsic haptic displays. In this paper we discuss some of the implications, advantages and limitations of such intrinsic haptic displays and present a range of novel applications that implement intrinsic haptic displays by using REVEL technology. Exploring the concept of intrinsic tactile displays in the context of AR environments and applications is another important contribution of this paper.", 
        "id": "siggraph45", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph52": {
        "time": 52, 
"html": "<b>What Makes Paris Look like Paris?</b><br><br> #Authors: Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei A. Efros<br><br> #Introduction #Paragraph1: Consider the two photographs in Figure 1, both downloaded from Google Street View. One comes from Paris, the other one from London. Can you tell which is which? Surprisingly, even for these nondescript street scenes, people who have been to Europe tend to do quite well on this task. In an informal survey, we presented 11 subjects with 100 random Street View images of which 50% were from Paris, and the rest from eleven other cities. We instructed the subjects (who have all been to Paris) to try and ignore any text in the photos, and collected their binary forced-choice responses (Paris / Not Paris). On average, subjects were correct 79% of the time (std = 6:3), with chance at 50% (when allowed to scrutinize the text, performance for some subjects went up as high as 90%). What this suggests is that people are remarkably sensitive to the geographically-informative features within the visual environment. But what are those features? In informal debrie?ngs, our subjects suggested that for most images, a few localized, distinctive elements ?immediately gave it away?. E.g. for Paris, things like windows with railings, the particular style of balconies, the distinctive doorways, the traditional blue/green/white street signs, etc. were particularly helpful. Finding those features can be dif?cult though, since every image can contain more than 25;000 candidate patches, and only a tiny fraction will be truly distinctive. ", 
        "id": "siggraph52", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph51": {
        "time": 51, 
"html": "<b>What Makes Paris Look like Paris?</b><br><br> #Authors: Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei A. Efros<br><br> #Introduction #Paragraph2: In this work, we want to ?nd such local geo-informative features automatically, directly from a large database of photographs from a particular place, such as a city. Speci?cally, given tens of thousands of geo-localized images of some geographic region R, we aim to ?nd a few hundred visual elements that are both: 1) repeating, i.e. they occur often in R, and 2) geographically discriminative, i.e. they occur much more often in R than in R C. Figure 1 shows sample output of our algorithm: for each photograph we show three of the most geo-informative visual elements that were automatically discovered. For the Paris scene (left), the street sign, the window with railings, and the balcony support are all ?agged as informative.", 
        "id": "siggraph51", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph50": {
        "time": 50, 
"html": "<b>What Makes Paris Look like Paris?</b><br><br> #Authors: Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei A. Efros<br><br> #Introduction #Paragraph3: But why is this topic important for modern computer graphics? 1) Scienti?cally, the goal of understanding which visual elements are fundamental to our perception of a complex visual concept, such as a place, is an interesting and useful one. Our paper shares this motivation with a number of other recent works that don?t actually synthesize new visual imagery, but rather propose ways of ?nding and visualizing existing image data in better ways, be it selecting candid portraits from a video stream [Fiss et al. 2011], summarizinga scene from photo collections [Simon et al. 2007], ?nding iconic images of an object [Berg and Berg 2009], etc. 2) More practically, one possible future application of the ideas presented here might be to help CG modelers by generating so-called ?reference art? for a city. For instance, when modeling Paris for PIXAR?s Ratatouille, the co-director Jan Pinkava faced exactly this problem: ?The basic question for us was: ?what would Paris look like as a model of Paris??, that is, what are the main things that give the city its unique look?? [Paik 2006]. Their solution was to ?run around Paris for a week like mad tourists, just looking at things, talking about them, and taking lots of pictures? not just of the Eiffel Tower but of the many stylistic Paris details, such as signs, doors etc. [Paik 2006](see photos on pp.120?121). But if going ?on location? is not feasible, our approach could serve as basis for a detail-centric reference art retriever, which would let artists focus their attention on the most statistically signi?cant stylistic elements of the city. 3) And ?nally, more philosophically, our ultimate goal is to provide a stylistic narrative for a visual experience of a place. Such narrative, once established, can be related to others in a kind of geo-cultural visual reference graph, highlighting similarities and differences between regions. E.g. one could imagine finding a visual appearance ?trail? from Greece, through Italy and Spain and into Latin America. In this work, we only take the ?rst steps in this direction ? connecting visual appearance across cities, ?nding similarities within a continent, and differences between neighborhoods. But we hope that our work might act as a catalyst for research in this new area, which might be called computational geo-cultural modeling.", 
        "id": "siggraph50", 
        "parent": "none", 
        "creator": "siggraph"
    }
}












 /*,
    "siggraph7": {
        "time": 7, 
        "html": "<b>Sketch-Based Shape Retrieval</b><br><br> #Authors: Mathias Eitz, Ronald Richter, Tamy Boubekeur, Kristian Hildebrand and Marc Alexa<br><br> #Introduction : Working with large collections of 3D models requires fast contentbased retrieval techniques, especially since public collections are often insufficiently annotated. In that case a keyword based search alone is not promising. While research on example-based retrieval ? where users provide a full model as the query ? has recently found a lot of interest in the community [Tangelder and Veltkamp 2008], its practical application is dif?cult, since a good example is often not at hand. Instead, sketch-based retrieval has been proposed [Lof?er ¨2000; Funkhouser et al. 2003; Chen et al. 2003; Yoon et al. 2010;Shao et al. 2011], where users sketch the desired model as seenfrom one or more viewpoints. We consider sketch-based retrieval to be even more challenging than example-based retrieval as the querycontains only partial information about the projection of the shape.Most humans have limited drawing skills and lines may additionally deviate significantly from that projection. These properties of the input directly translate into desiderata of sketch-based shape retrieval systems: a) partial matching of feature lines of the shape in b) all potential viewing directions to the sketch, tolerating c) global and local deformation; and, clearly, the retrieval performance has to d) scale to large collections. We present, to our knowledge, the ?rst approach that addresses all of the desiderata. The approach is based on the visual analysis of meshes: we sample the set of likely view directions, generate line drawings with state of the art line rendering techniques, and encode the line drawings with a bag-of-features approach. This choice is directly related to the requirements. First, rather than trying to match projected lines to shape features in 3D, we exploit current line art rendering techniques. They have reached a mature state, in which almost all lines drawn by humans are also generated by algorithms [Cole et al. 2008]. Second, bag-of-features approaches, which are well known in the image retrieval community [Sivic and Zisserman 2003], use local image descriptors that are independent of location. This is ideal, as it immediately enables partial matching and is resilient to global deformations. We achieve additional resilience to local deformations by quantization of the local image descriptors (identifying so-called ?visual words?) and matching based on histograms. This data reduction leads, third, to the desired fast query times. We make use of successful techniques from other domains where appropriate and provide the following novel contributions: A large-scale benchmark for sketch-based retrieval systems. The benchmark is based on a real-world dataset of 1,914 sketches gathered from a large variety of participants in a perceptual experiment. We provide this dataset as a free resource. A new feature transform based on a bank of Gabor ?lters that is tuned to the requirements of sketch-based shape retrieval. This descriptor outperforms other existing transformations. A general approach to determine optimal parameters for such feature transformations. We demonstrate that even existing systems can be improved using this approach. Overall, this leads to a system with high quality retrieval performance as we demonstrate in our objective evaluation as well as the accompanying video using a large variety of real-world user sketches. We also demonstrate the power of our system in Fig. 1 where we gather all objects for a complete scene in about two minutes. However, we also ?nd that the real-world dataset of sketches gathered in the experiment is challenging for current systems. In particular, our dataset reveals that allowing only closed contour curves for retrieval [Chen et al. 2003] oversimpli?es reality: a large majority of our participants? sketches contain a substantial amount of interior lines. The insights gained from an analysis of our dataset open up several promising areas of further research which we identify in Sec. 8.", 
        "id": "siggraph7", 
        "parent": "none", 
        "creator": "siggraph"
    }, 
    "siggraph8": {
        "time": 8, 
        "html": "<b>How Do Humans Sketch Objects?</b><br><br> #Authors: Mathias Eitz, James Hays and Marc Alexa<br><br> #Introduction : Sketching is a universal form of communication. Since prehistoric times, people have rendered the visual world in sketch-like petroglyphs or cave paintings. Such pictographs predate the appearance of language by tens of thousands of years and today the ability to draw and recognize sketched objects is ubiquitous. In fact, recent neuroscience work suggests that simple, abstracted sketches activate our brain in similar ways to real stimuli [Walther et al. 2011]. Despite decades of graphics research, sketching is the only mechanism for most people to render visual content. However, there has never been a formal study of how people sketch objects and how well such sketches can be recognized by humans and computers. We examine these topics for the ?rst time and demonstrate applications of computational sketch understanding. In this paper we use the term ?sketch? to mean a non-expert, abstract pictograph and not to imply any particular medium (e.g. pencil and paper). There exists signi?cant prior research on retrieving images or 3d models based on sketches. The assumption in all of these works is that, in some well-engineered feature space, sketched objects resemble their real-world counterparts. But this fundamental assumption is often violated ? most humans are not faithful artists. Instead people use shared, iconic representations of objects (e.g. stick ?gures) or they make dramatic simpli?cations or exaggerations (e.g. pronounced ears on rabbits). Thus to understand and recognize sketches an algorithm must learn from a training dataset of real sketches, not photos or 3d models. Because people represent the same object using differing degrees of realism and distinct drawing styles (see Fig. 1), we need a large dataset of sketches which adequately samples these variations. There also exists prior research in sketch recognition which tries to identify prede?ned glyphs in narrow domains (e.g. wire diagrams, musical scores). We instead identify objects such as snowmen, ice cream cones, giraffes, etc. This task is hard, because an average human is, unfortunately, not a faithful artist. Although both shape and proportions of a sketched object may be far from that of the corresponding real object, and at the same time sketches are an impoverished visual representation, humans are amazingly accurate at interpreting such sketches. We ?rst de?ne a taxonomy of 250 object categories and acquire a large dataset of human sketches for the categories using crowdsourcing (Sec. 3). Based on the dataset we estimate how humans perform in recognizing the categories for each sketch (Sec. 4). We design a robust visual feature descriptor for sketches (Sec. 5). This feature permits not only unsupervised analysis of the dataset (Sec. 6), but also the computational recognition of sketches (Sec. 7). While we achieve a high computational recognition accuracy of 56% (chance is 0.4%), our study also reveals that humans still perform signi?cantly better than computers at this task. We show several interesting applications of the computational model (Sec. 8): apart from the interactive recognition of sketches itself, we also demonstrate that recognizing the category of a sketch could improve image retrieval. We hope that the use of sketching as a visual input modality opens up computing technology to a signi?cantly larger user base than text input alone. This paper is a ?rst steptoward this goal and we release the dataset to encourage future research in this domain.", 
        "id": "siggraph8", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph9": {
        "time": 9, 
        "html": "<b>Tools for Placing Cuts and Transitions in Interview Video</b><br><br> #Authors: Floraine Berthouzoz, Wilmot Li, Maneesh Agrawala<br><br> #Introduction : Interview video frequently appears in broadcast news, documentary films, and online lectures. Such video usually focuses on the head and upper body of a person who explains a concept while looking at the camera. The challenge in editing interview video is to convert hours of raw footage into a concise clip that conveys the desired story. The process involves two steps: (1) choosing where to cut segments of the raw footage and (2) creating audio/visual transitions that connect the remaining segments into the ?nal clip. To place a cut, the editor must consider both the semantic content and the audio/visual attributes of the raw footage. Editors often begin by watching the entire raw footage several times and manually building an index that describes its content. Once the editor decides on the story, he can use the index to ?nd the segments corresponding to speci?c content. The editor must also account for the audio/visual attributes of the segments when deciding where to cut. For example, cutting while the speaker is in the middle of saying a word or is gesturing energetically is often undesirable because such cuts interrupt the audio/visual flow of the video [O?Steen 2009]. After choosing the rough position of a cut the editor must create a transition that connects the video segments before and after the cut. Such transitions fall into two main categories; Hidden Transitions conceal the cut by smoothly joining the segments. Thus, they allow viewers to remain focused on the continuous ?ow of semantic content in the video. Visible Transitionsintroduce a noticeable change in audio/visual content (e.g. jump-cut, fade, etc.) between the segments. They usually indicate a break in the ?ow of semantic content. Producing hidden transitions can be dif?cult as it requires either synthetically interpolating new frames or ?nding existing frames within the footage to seamlessly join the segments. Yet, interpolation [Mahajan et al. 2009; Brox et al. 2004] is only effective when the endpoint frames are similar in appearance, and manually searching for existing frames in the entire raw footage is impractical. As a result, editors often end up using visible transitions to join segments even when there is no break in the ?ow of semantic content. In this work, we present tools for interactively placing cuts and creating hidden as well as visible transitions in interview video footage. To help editors place the cuts, we combine crowdsourcing (castingwords.com) to obtain a text transcript of the raw footage, with text-audio alignment software [Virage ] to build an index that links each word in the transcript with the corresponding location in the footage. The editor can then directly edit the text script and our tools propagate the changes to the video. We also visualize the suitability of cut locations by analyzing the audio/visual features of the raw footage to ?nd frames where the speaker is relatively quiet and still (Figure 1). Together these tools allow the editor to focus on the semantic content and quickly select cut locations that do not interrupt the ?ow of the video in an unnatural manner. For each cut, our system automatically generates hidden and visible transitions so that the editor can see each possibility and choose whichever one is most appropriate. We present a new algorithm for efficiently generating hidden transitions. Our method builds on the general approach of constructing a similiarity graph between (b) Removing a phrase with low suitability (c) Removing ums and repeated words the raw frames and walking the graph to select the smoothest set of in-between frames [Schodl et al. 2000; Kwatra et al. 2003; ¨ Kemelmacher-Shlizerman et al. 2011]. Unique to our approach is a frame matching distance that considers facial appearance, body appearance, and location of the speaker within the frame. We also use hierarchical clustering to signi?cantly reduce the number of frameto-frame comparisons necessary to achieve good performance. To further re?ne the resulting transition we apply optical ?ow interpolation [Brox et al. 2004] with a new data-driven technique for selecting the number of interpolated frames. Our technique sets the timing of the transitions so that any motions of the speaker appear natural. Such timing is crucial for maintaining the ?ow of the video. Beyond placing simple cuts and transitions, our interface offers additional tools for increasing the quality of the ?nal clip. The transcript view highlights ?ums? and repeated words and gives editors a one click option for seamlessly removing them. Editors can also insert natural-looking pauses, in which the speaker appears to be at rest, to further emphasize important semantic content. We apply our tools on videos from a variety of sources including a corporate marketing department and professional journalists. In addition to interview editing, we show how our interface can be used to quickly compose multiple takes of an actor narrating a story. Finally, we demonstrate the success of our approach by providing visual comparisons to a number of other techniques.", 
        "id": "siggraph9", 
        "parent": "none", 
        "creator": "siggraph"
    },
     "siggraph10": {
        "time": 10, 
"html": "<b>Revel: Tactile Feedback Technology for Augmented Reality</b><br><br> #Authors: Olivier Bau, ivan poupyrev<br><br> #Introduction : Augmented Reality has recently emerged as one of the key application areas of interactive computer graphics and is rapidly expanding from research laboratories into everyday use. The fundamental premise of AR is to enable us to interact with virtual objects immediately and directly, seeing, feeling and manipulating them just as we do with physical objects. Most AR applications, however, provide only visual augmentation of the real world and do not provide the means to let the user feel tactile, physical properties of virtual objects or to enhance the physical world with computer-generated tactile textures. The absence of tactile feedback does not allow us to take advantage of the powerful mechanisms of the human sense of touch and diminishes the quality of the experience. REVEL is an augmented reality (AR) tactile technology that allows us to change the tactile feeling of real world objects by augmenting them with virtual tactile textures(Figure 1). It is based on the principle of reverse electrovibration where a weak electrical signal is injected anywhere on the user?s body, creating an oscillating electrical field around the user?s fingers. As the user then slides his or her fingers on the surface of an object, the user perceives highly distinctive tactile textures overlying the physical object. By tracking touch locations, tactile textures can be dynamically modified in real time and enhanced with visual augmentation if required. The user?s hands remain free and unencumbered, so that users can continue their natural interaction with the world around them, unconstrained by tactile feedback technology. In a broad sense we are programmatically controlling the user?stactile perception. REVEL advances our previous research on electrovibration-based tactile displays for touch surfaces, i.e., TeslaTouch [Bau, Poupyrev, et al. 2010]. In TeslaTouch the electrical signal was injected into the surface electrode of the touch screens, the classic technique to design tactile displays based on electrovibration, e.g., devices for the blind proposed in the early 70s [Strong, 1970]. In all these displays, including TeslaTouch, the tactile sensation is localized within a specific device augmented with tactile feedback, which is not scalable. Indeed, to add virtual tactile sensations to more objects or devices, all of them must be instrumented with tactile feedback apparatus. REVEL produces the same tactile effect, but reverses this dependency on individual object instrumentation. It instead injects tactile signals directly into the user?s body, so that the user becomes the carrier of the tactile signal at all times. The world and objects remain passive, requiring no instrumentation with additional technology. Therefore, this technology potentially allows for the creation of truly ubiquitous tactile interfaces that can be used anywhere and anytime. The development of reverse electrovibration-based tactile displays requires an in-depth understanding of the basic physical principles of electrovibration. It involves the investigation of the interaction of electrostatic fields with human body, environment and various materials, as well as the exploration of the effect of their various properties on tactile sensations. Developing this body of knowledge and techniques that allow designing tactile feedback ?in the world? is one of the important contributions of this paper that advances the state of the art in electrovibration-based tactile interfaces. In designing REVEL we were also motivated by a broader vision of tactile displays that presents tactile stimuli by altering user tactile perception [Kruijff et al. 2006]. Such tactile displays do not require environmental instrumentation and could be used anywhere, leading to the design of a truly ubiquitous tactile augmented reality as envisioned by early AR pioneers [Azuma 1997]. We refer to this new class of tactile devices as intrinsic haptic displays. In this paper we discuss some of the implications, advantages and limitations of such intrinsic haptic displays and present a range of novel applications that implement intrinsic haptic displays by using REVEL technology. Exploring the concept of intrinsic tactile displays in the context of AR environments and applications is another important contribution of this paper.", 
        "id": "siggraph10", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph11": {
        "time": 11, 
"html": "<b>What Makes Paris Look like Paris?</b><br><br> #Authors: Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei A. Efros<br><br> #Introduction : Consider the two photographs in Figure 1, both downloaded from Google Street View. One comes from Paris, the other one from London. Can you tell which is which? Surprisingly, even for these nondescript street scenes, people who have been to Europe tend to do quite well on this task. In an informal survey, we presented 11 subjects with 100 random Street View images of which 50% were from Paris, and the rest from eleven other cities. We instructed the subjects (who have all been to Paris) to try and ignore any text in the photos, and collected their binary forced-choice responses (Paris / Not Paris). On average, subjects were correct 79% of the time (std = 6:3), with chance at 50% (when allowed to scrutinize the text, performance for some subjects went up as high as 90%). What this suggests is that people are remarkably sensitive to the geographically-informative features within the visual environment. But what are those features? In informal debrie?ngs, our subjects suggested that for most images, a few localized, distinctive elements ?immediately gave it away?. E.g. for Paris, things like windows with railings, the particular style of balconies, the distinctive doorways, the traditional blue/green/white street signs, etc. were particularly helpful. Finding those features can be dif?cult though, since every image can contain more than 25;000 candidate patches, and only a tiny fraction will be truly distinctive. In this work, we want to ?nd such local geo-informative features automatically, directly from a large database of photographs from a particular place, such as a city. Speci?cally, given tens of thousands of geo-localized images of some geographic region R, we aim to ?nd a few hundred visual elements that are both: 1) repeating, i.e. they occur often in R, and 2) geographically discriminative, i.e. they occur much more often in R than in R C. Figure 1 shows sample output of our algorithm: for each photograph we show three of the most geo-informative visual elements that were automatically discovered. For the Paris scene (left), the street sign, the window with railings, and the balcony support are all ?agged as informative. But why is this topic important for modern computer graphics? 1) Scienti?cally, the goal of understanding which visual elements are fundamental to our perception of a complex visual concept, such as a place, is an interesting and useful one. Our paper shares this motivation with a number of other recent works that don?t actually synthesize new visual imagery, but rather propose ways of ?nding and visualizing existing image data in better ways, be it selecting candid portraits from a video stream [Fiss et al. 2011], summarizinga scene from photo collections [Simon et al. 2007], ?nding iconic images of an object [Berg and Berg 2009], etc. 2) More practically, one possible future application of the ideas presented here might be to help CG modelers by generating so-called ?reference art? for a city. For instance, when modeling Paris for PIXAR?s Ratatouille, the co-director Jan Pinkava faced exactly this problem: ?The basic question for us was: ?what would Paris look like as a model of Paris??, that is, what are the main things that give the city its unique look?? [Paik 2006]. Their solution was to ?run around Paris for a week like mad tourists, just looking at things, talking about them, and taking lots of pictures? not just of the Eiffel Tower but of the many stylistic Paris details, such as signs, doors etc. [Paik 2006](see photos on pp.120?121). But if going ?on location? is not feasible, our approach could serve as basis for a detail-centric reference art retriever, which would let artists focus their attention on the most statistically signi?cant stylistic elements of the city. 3) And ?nally, more philosophically, our ultimate goal is to provide a stylistic narrative for a visual experience of a place. Such narrative, once established, can be related to others in a kind of geo-cultural visual reference graph, highlighting similarities and differences between regions. E.g. one could imagine finding a visual appearance ?trail? from Greece, through Italy and Spain and into Latin America. In this work, we only take the ?rst steps in this direction ? connecting visual appearance across cities, ?nding similarities within a continent, and differences between neighborhoods. But we hope that our work might act as a catalyst for research in this new area, which might be called computational geo-cultural modeling.", 
        "id": "siggraph11", 
        "parent": "none", 
        "creator": "siggraph"
    },
    
    
    
    
    
    "siggraph12": {
        "time": 12, 
"html": "<b>HelpingHand: Example-based Stroke Stylization</b><br><br> #Authors: Jingwan Lu, Fisher Yu, Adam Finkelstein, Stephen DiVerdi<br><br> #Introduction : Artists wielding traditional drawing instruments such as a pencil, a stick of charcoal or a paint brush exercise fine control over the interplay between their gestures and the physical medium. The appearance of the strokes is governed not only by the path of the instrument but also the pressure and angle (see inset). Digital artists often work with high quality tablets that track a stylus with 6 DOFs (2D position, pressure, 2D tilt, and rotation), which painting software can use to simulate the interaction of digital brushes in various media with increasing fidelity [Baxter and Lin 2004]. However, most people drawing on digital devices today do not use 6-DOF tablets, for two reasons. First, high-quality tablets are relatively expensive as compared with other input devices like mice (2-DOF), multitouch screens (2-DOF), or mass-market tablets (3- or 5-DOF). Second, even if everyone owned a 6-DOF tablet, most people would not have the training and experience to be able to wield it effectively. To achieve high quality calligraphy, for example, takes many hours (perhaps years) of practice. This paper describes a method whereby a non-expert draws a 2D query stroke (Figure 1a), perhaps using an inexpensive input device, and missing DOFs are synthesized based on a library of examples supplied by an artist (Figure 2a). The resulting marks follow the trajectory drawn by the non-expert but convey the gestural style of the artist (Figure 1b). We also show how the approach can be extended to approximate the input trajectory while adopting some of the character of the strokes in the artist?s library (Figure 1c). This option is beneficial for users who are not confident in their own style, or when using a coarse input device like a mouse. One can easily change style by choosing a different library (Figure 1d). To synthesize hand pose, we address this problem: given a library of 6-DOF strokes (drawn by the artist) and a 2-DOF query stroke (drawn by the user), produce 4-DOFs to accompany the query so that the 6-DOF (2+4) combination looks like the strokes in the library. Trajectory stylization optionally replaces the 2-DOF trajectory as well. There is no single correct answer because the same path can be drawn differently, even by a single artist, so our goal is to synthesize plausible gestures. Our method produces gestures that are plausibly in the style of the artist?s library. It performs at interactive rates to provide immediate feedback to the user during drawing and to make it easy to choose amongst different styles. Finally, while we have designed the system for interactive use by non-experts or people without highquality tablets, the same method can apply stylization to any source of line art, including the output of vector illustration software or computer-generated line drawings based on 3D models. The main contributions of this paper include stroke stylization via example-based synthesis of hand pose?data needed to control a virtual brush?as well as optional trajectory modification. Through data collection and analysis we offer understanding of how hand pose relates to individual writing style. Finally, we explore the space of candidate algorithms and evaluate them quantitatively and qualitatively, concluding with a demonstration of practical results. #Image <img src='http://homes.cs.washington.edu/~felicia0/images/twitify/siggraph12.png' width='415px'>", 
        "id": "siggraph12", 
        "parent": "none", 
        "creator": "siggraph"
    },
    "siggraph13": {
        "time": 13, 
"html": "<b>EXPLORING COLLECTIONS OF 3D MODELS USING FUZZY CORRESPONDENCES</b><br><br> #Authors: Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen DiVerdi, Thomas Funkhouser <br><br> #Introduction: 								Increasing availability of powerful modeling software and 3D acquisition devices has led to rapidly growing repositories of 3D models (e.g., TurboSquid, Google 3D Warehouse, etc.). Yet, the task of exploring such large 3D repositories remains an important and challenging problem. In particular, while most online databases make it easy for users to select sets of similar models (which we refer to as model collections) using text-based ?ltering, understanding the range of variations within such collections is typically much more dif?cult (see also [Ovsjanikov et al. 2011]). For many object classes, one key challenge is that the shape can vary in many different ways, and users may be interested in exploring different types of variations. For example, within a collection of chair models, one user may want to see how the backs of chairs attach to the seat (smoothly merge, right angle, etc.) while another user may only want to see chairs whose back legs are at a certain angle (see Figure 1). Even within a single exploration session, a user may want to de?ne the exploration space using multiple different attributes (e.g., chairs with a stem base and curved back). Given the spectrum of different possible exploration criteria, a static prede?ned organization of the data is clearly not suf?cient. This is especially true for very large and diverse collections, like the ones we ?nd for many object classes in the Google 3D Warehouse (e.g., chairs, cars, animals). In this work, we present a new analysis tool and exploration interface for 3D model collections. As a key feature, we allow users to directly specify regions of interest (ROI) on example shapes in order to guide subsequent exploration actions. Thus we can support the browsing scenarios described above; the user selects the appropriate ROIs on one or more chairs, and the system automatically organizes the rest of the chairs based on their similarity to the speci?ed region. From a Human Computer Information Retrieval perspective, our system can be described as a type offaceted browsing interface, which has proven to be a very effective and popular method for exploring diverse collections of items with a wide variety of attributes [Hearst 2006]. For example, faceted interfaces are very common for online shopping websites where attributes like price, date of release, popularity, etc. act as prede?ned facets for navigation. In contrast, our facets are de?ned interactively as the user selects speci?c ROIs (see supplementary video). The main technical challenge in realizing our proposed interface is how to relate any arbitrary user selected region on one shape to all the other models in the collection. Since user-selected ROIs do not necessarily match pre-segmented parts, consistent part-level segmentation is not suf?cient. Rather, we prefer correspondences for individual points. Yet, automatically establishing one-to-one point correspondences across a model collection is dif?cult and often ambiguous, especially when the collection includes diverse shapes. For example, how do the chair bases in the the top row of Figure 1 correspond to the chair legs in the bottom row? To address this issue, we encode the ambiguity of correspondences in model collections with fuzzy correspondences1 . Speci?cally, given a collection of N shapes S := fS1;S2; : : : ;SNg, we use fuzzy correspondences as a function f(pi ; pj) : S  S ! R to denote a continuous similarity measure between points pi 2 Sm and pj 2 Sn. To estimate fuzzy correspondences, f(pi ; pj), we utilize geometric matching methods that align pairs of shapes. Although these methods are computationally expensive and often produce noisy alignments, we observe that for collections of shapes from the same class a correspondence matrix that stores high values for corresponding pairs of points is (i) sparse, (ii) low-rank, and (iii) its rank does not depend on the number of models. We propose a method based on diffusion maps [Nadler et al. 2006] to reconstruct f from sparse and noisy samples (i.e., pairwise alignments) and an iterative procedure to re?ne f by adaptively sampling based on the current estimate. We test the accuracy of our estimate of f using the correspondence benchmark for intrinsically-similar shapes [Kim et al. 2011]. We also introduce a new correspondence benchmark of 111 chairs and 86 commercial airplanes using data obtained from the Google 3D Warehouse. Our method successfully utilizes the collection to improve alignments of shapes in comparison to existing methods (see Figures 11, 12, and supplementary material). Contributions. In summary, we  introduce an approach for using fuzzy correspondences to understand similarity relations across 3D model collections,  propose a robust and ef?cient algorithm to compute fuzzy correspondences from sparse and noisy pairwise alignments,  evaluate our algorithm on correspondence benchmarks and report substantial improvement over existing alternatives, and  present an interactive exploration tool for large model collections that uses fuzzy correspondences to support view alignment, ROI-based similarity search, and faceted exploration.", 
        "id": "siggraph13", 
        "parent": "none", 
        "creator": "siggraph"
    }*/











