completionCondition=[
	{
	"number1":"all",
	"attribute1":"discussion",
	"number2":"2",
	"attribute2":"hashtags"
	}
] 

tweets = {
    "uist0": {
        "time": 0, 
        "html": "<b>Gesture Output: Eyes-Free Output: Using a #Force Feedback Touch Surface</b><br><br>We propose using spatial gestures not only for input but also for output. Analogous to gesture input, gesture output moves the user\u2019s finger in a gesture, which the user then recognizes. A motion path forming a \u201c5\u201d, for example, may inform the user about five unread messages; a heart-shaped path may serve as a message from a close friend. We built two prototypes: (1) The longRangeOuija is a stationary prototype that offers a large motion range and full control We used it to conduct user studies. (2) The pocketOuija is self-contained mobile device based on an iPhone. It actuates the user\u2019s fingers by means of a transparent foil overlaid onto the screen using motors located on the back of the device. We present three user studies, in which participants recognized marking output with 97% accuracy, Graffiti digits with 98.8%, compound pairs of Graffiti digits with 90.5%, and Graffiti letters with 93.4%. Participants previously unfamiliar with Graffiti correctly identified 96.2% of digits and 76.4% of letters, suggesting that properly designed gesture output is guessable. After the experiment, the same participants were able to enter 100% of Graffiti digits by heart and 92.2% of Graffiti letters. This suggests that gesture output teaches gesture input as a side effect.", 
        "id": "uist0", 
        "parent": "none", 
        "creator": "uist"
    }, 
    "uist1": {
        "time": 1, 
        "html": "<b>iMuscle: Mobile Force-Feedback based on Electrical Muscle Stimulation</b><br><br>Unlike many other areas in computer science and HCI, force-feedback devices resist miniaturization, because they require physical motors. We propose mobile force-feedback devices based on actuating the user\u2019s muscles using electri-cal stimulation. Since this allows us to eliminate motors and reduce battery size, our approach results in devices that are substantially smaller and lighter than traditional motor-based devices. We present a simple prototype that we mount to the back of a mobile phone. It actuates users\u2019 forearm muscles via four electrodes, causing the muscles to contract involuntarily, so that users tilt the device sideways. As users resist this motion using their other arm, they per-ceive force feedback. We demonstrate the interaction at the example of an interactive videogame in which users try to steer an airplane through winds rendered using force-feedback.  \\  \\ We evaluate our approach in 3 simple experiments. In Study 1, we measured that the device delivers up to 17.5N of force when applied to the palm flexor. In Study 2, partic-ipants correctly identified force feedback direction with 97.7% accuracy eyes-free. Finally, in Study 3 participants reported their experiences playing the video game de-scribed above. \\ ", 
        "id": "uist1", 
        "parent": "none", 
        "creator": "uist"
    }, 
    "uist2": {
        "time": 2, 
        "html": "<b>Morphees: #Force Toward High \u201cShape Resolution\u201d in Self-Actuated Flexible Mobile Devices</b><br><br>We introduce the term shape resolution, which adds to the existing definitions of screen and touch resolution. We propose a framework, based on a geometric model (Non-Uniform Rational B-splines), which defines a metric for shape resolution in ten features. We illustrate it by comparing the current related work of shape changing devices. We then propose the concept of Morphees, an unexplored area that our framework reveals. Morphees are self-actuated flexible mobile devices that adapt their shapes on their own to the context of use in order to offer better affordances. When a game is launched, the mobile device for instance morphs into a console-like shape by curling two opposite edges and be better grasped with two hands. We then built six strategies of creating Morphees using advanced shape changing materials (dielectric electro active polymers and shape memory alloys). By comparing them with our framework, we generate insights to help designers toward creating high shape resolution Morphees.", 
        "id": "uist2", 
        "parent": "none", 
        "creator": "uist"
    }, 
    "uist3": {
        "time": 3, 
        "html": "<b>Collaborative Sensemaking on a Digital Tabletop and Personal Tablets: Prioritization, Comparisons, and Tableaux</b><br><br>We describe an investigation of technological support for a collaborative sensemaking task. In particular, we investi- gated the support that three different display configurations provided: a digital table; personal tablets; and both the table- top and personal tablets. Mixed-methods analyses revealed that the presence of a digital tabletop display led to improved sensemaking performance, and identified activities that were supported by the shared workspace. The digital tabletop sup- ported a group\u2019s ability to prioritize information, to make comparisons between task data, and to form and critique the group\u2019s working hypothesis. Analyses of group performance revealed a positive correlation with equity of member participation using the shared digital table, and a negative correlation of equity of member participation using personal tablets. Implications for the support of sensemaking groups, and the use of equity of member participation as a predictive measure of their performance are discussed.", 
        "id": "uist3", 
        "parent": "none", 
        "creator": "uist"
    }, 
    "uist4": {
        "time": 4, 
        "html": "<b>How Tools in IDEs Shape Developers\u00b4 Navigation Behavior</b><br><br>Understanding source code is crucial for successful software maintenance, and navigating the call graph is especially helpful to understand source code. \\ We compared maintenance performance across four different development environments: an IDE without any call graph exploration tool, a Call Hierarchy tool as found in Eclipse, and the tools Stacksplorer and Blaze. \\ Using any of the call graph exploration tools more developers could solve certain maintenance tasks correctly.  \\ Only Stacksplorer and Blaze, however, were also able to decrease task completion times, although the Call Hierarchy offers access to a larger part of the call graph.  \\ To investigate if this result was caused by a change in navigation behavior between the tools, we used a set of predictive models to create formally comparable descriptions of programmer navigation.  \\ The decrease in task completion times may have been caused by Stacksplorer and Blaze promoting call graph navigation more than the Call Hierarchy tool.", 
        "id": "uist4", 
        "parent": "none", 
        "creator": "uist"
    }, 
    "viz0": {
        "time": 0, 
        "html": "<b>Personal Clipboards for Individual Copy-and-Paste on Shared Multi-User Surfaces</b><br><br>Clipboards are omnipresent on today\u2019s personal computing \\ platforms. They provide copy-and-paste functionalities that \\ let users easily reorganize information and quickly transfer \\ data across applications. In this work, we introduce personal \\ clipboards to multi-user surfaces. Personal clipboards enable \\ individual and independent copy-and-paste operations, \\ in the presence of multiple users concurrently sharing the \\ same direct-touch interface. As common surface computing \\ platforms do not distinguish touch input of different users, \\ we have developed clipboards that leverage complementary \\ personalization strategies. Specifically, we have built a context \\ menu clipboard based on implicit user identification of \\ every touch, a clipboard based on personal subareas dynamically \\ placed on the surface, and a handheld clipboard based \\ on integration of personal devices for surface interaction. In \\ a user study, we demonstrate the effectiveness of personal \\ clipboards for shared surfaces, and show that different personalization \\ strategies enable clipboards, albeit with different \\ impacts on interaction characteristics.", 
        "id": "viz0", 
        "parent": "none", 
        "creator": "viz"
    }
    , 
    "viz3": {
        "time": 3, 
        "html": "<b>MorePhone: A Study of Actuated Shape Deformations for Flexible Thin-Film Smartphone Notifications</b><br><br>We present MorePhone, an actuated flexible smartphone with a thin-film E Ink display. MorePhone uses shape memory alloys to actuate the entire surface of the display as well as individual corners. We conducted a participatory study to determine how users associate urgency and notification type with full screen, 1 corner, 2 corner and 3 corner actuations of the smartphone. Results suggest that in the current prototype, actuated shape notifications are most useful for visual feedback. Shapes that actuated a larger surface area were ranked as significantly more urgent. Urgent notifications such as alarms and voice calls were best matched with actuation of the entire display surface, while less urgent notifications, such as text messages, best matched to individual corner bends. While different corner actuations resulted in significantly different matches between notification types, most notification types were treated as similar, and best matched to a single corner bend. A follow-up study suggested that users prefer to dedicate each corner of the device to a specific type of notification, rather than using the shape of the entire device to determine notification type. Animation of shape actuation significantly increased the perceived urgency of any of the presented shapes.", 
        "id": "viz3", 
        "parent": "none", 
        "creator": "viz"
    }, 
    "viz2": {
        "time": 2, 
        "html": "<b>Evaluating the Efficiency of Physical Visualizations</b><br><br>Data sculptures are an increasingly popular form of physical visualization whose purposes are essentially artistic, communicative or educational. But can physical visualizations help carry out actual information visualization tasks? We present the first infovis study comparing physical to on-screen visualizations. We focus on 3D visualizations, as these are common among physical visualizations but known to be problematic on computers. Taking 3D bar charts as an example, we show that moving visualizations to the physical world can improve users' efficiency at information retrieval tasks. In contrast, augmenting on-screen visualizations with stereoscopic rendering alone or with prop-based manipulation was of limited help. The efficiency of physical visualizations seems to stem from features that are unique to physical objects, such as their ability to be touched and their perfect visual realism. These findings provide empirical motivation for current research on fast digital fabrication and self-reconfiguring materials.", 
        "id": "viz2", 
        "parent": "none", 
        "creator": "viz"
    }, 
    "viz4": {
        "time": 4, 
        "html": "<b>Focus+Magic: Peripheral Projected Illusions for Interactive Experiences</b><br><br>Focus+Magic is a proof-of-concept system that augments the area surrounding a television screen with projected visualizations to enhance traditional gaming experiences. We investigate how projected visualizations in the periphery can enhance, negate or distort the existing physical environment and thus enhance the content displayed on the television screen. Such peripheral projected illusions can change the appearance of the room, induce apparent motion, extend the field of view, and enable entirely new game experiences. Our system is self-calibrating and is designed to work in any living room. We present a detailed exploration of the design space of projected peripheral illusions that are possible with the Focus+Magic system and we demonstrate ways to trigger and drive such illusions. We also contribute specific feedback from two groups of target users (10 gamers and 15 game designers) providing insights for enhancing game experiences through peripheral illusions.", 
        "id": "viz4", 
        "parent": "none", 
        "creator": "viz"
    }, 
    "viz1": {
        "time": 1, 
        "html": "<b>Browsing Imaginary Interfaces: the Role of Visual and Tactile Cues when Exploring a Palm-based Interface</b><br><br>Imaginary Interfaces are screen-less ultra-mobile interfaces. Even though they offer no visual feedback, they were shown to allow users to interact spatially, e.g., by pointing at a location on their non-dominant hand. \\  \\ In this paper, we explore how to allow users to browse unfamiliar imaginary interfaces. We adapt an interface originally designed for visually impaired users to create a system that audibly announces target names as users scrub across their palm. In a user study, this interface allowed participants to familiarize themselves with an interface and locate targets reliably in 2.66s on average. \\  \\ Rather than just formally evaluating the interface, we investigated what caused its performance. We conducted three studies and found that (1) even though imaginary interfaces cannot display visual contents, users\u2019 visual sense remains the main stimulus that allows users to control the interface, as users watch their hands interact. (2) The tactile cues of both hands feeling each other in part replace the lacking visual cues when blindfolded, keeping imaginary interfaces usable. (3) While we initially expected the cues sensed by the pointing finger to be most important, we found instead that tactile cues sensed by the palm allow users to orient themselves. \\  \\ These findings primarily tell us about the nature of imaginary interfaces but also suggest that palm-based imaginary interfaces may have benefits for visually impaired users, potentially outperforming the touchscreen-based interaction they use today. We conclude by reporting experiences exploring this interaction style with one blind participant.", 
        "id": "viz1", 
        "parent": "none", 
        "creator": "viz"
    },
    "uist5": {
        "time": 5, 
        "html": "Id= uist5: reply to uist0 #foo", 
        "id": "uist5", 
        "parent": "uist0", 
        "creator": "hmslydia"
    }, 
    "uist8": {
        "time": 8, 
        "html": "Id= uist8: reply to uist5 #Foo", 
        "id": "uist8", 
        "parent": "uist5", 
        "creator": "hmslydia"
    }, 
    "uist6": {
        "time": 6, 
        "html": "reply to uist2 #bar ", 
        "id": "uist6", 
        "parent": "uist2", 
        "creator": "hmslydia"
    }, 
    "uist7": {
        "time": 7, 
        "html": "reply to uist6", 
        "id": "uist7", 
        "parent": "uist6", 
        "creator": "hmslydia"
    }, 
    "viz5": {
        "time": 5, 
        "html": "reply to viz0", 
        "id": "viz5", 
        "parent": "viz0", 
        "creator": "hmslydia"
    }, 
    "viz6": {
        "time": 6, 
        "html": "reply to viz2", 
        "id": "viz6", 
        "parent": "viz2", 
        "creator": "hmslydia"
    },   
    "viz7": {
        "time": 7, 
        "html": "reply to viz6", 
        "id": "viz7", 
        "parent": "viz6", 
        "creator": "asdf"
    }
    ,   
    "viz8": {
        "time": 7, 
        "html": "reply to viz6", 
        "id": "viz8", 
        "parent": "viz6", 
        "creator": "asdf"
    }
}
