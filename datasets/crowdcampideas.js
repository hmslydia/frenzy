ideasData = [{'idea': 'The motivational, cognitive and error diversity among human computation resources (a.k.a. humans ;-)) are hard problems to tackle in the design of human computation systems. Today, these kind of problems are mostly addressed using massive parallelism (e.g., as showed in reCaptcha).  I propose to think systematically on the design of coordination mechanisms to automatically adapt to the cognitive variance of human computation resources. To make the problem more tangible in a two day workshop, I propose to consider the problem in a particular domain such as monolingual translation process ("paraphrasing tasks") or speech-to-text translation. For  the implementation of these algorithms I can support the workshop team by providing the source code of CrowdLang (our human computation programming language) that allows to rapidly program new human computation systems (including the application logic, the design of user interfaces, the automated interactions with task markets).',
  'priority': '1'},
 {'idea': 'Memory Capsule: \x93Memory Capsule\x94 is an open system collecting memories of individuals in a community in digital forms. The foundation of collective social memory is a sum of  personal experiences among members of a community. Yet, we can only reproduce these memories in a limited way by analyzing historical writings and decontextualized museum pieces. \x93Memory Capsule\x94 uses digital sensors and instruments to record \x93memory\x94in various contexts by capturing images, audio, video, and so forth.  These contexts may include homes, restaurants, and other social sites. Contribution of data to the capsule is voluntary. After a set period of time, memory data will be made open to the \x93crowd\x94 for remixing or manipulation. For example, museums may retrieve \x93memories\x94 of monuments and historical events. Individuals may search for their family history. Besides, I\x92m also interested in the ways members in the public, or crowd, can utilize these memories of the past for the benefit of future society ',
  'priority': '1'},
 {'idea': "I am interested in citizen science related crowdsourcing. Citizen science is one kind of collaboration involving scientists and science enthusiasts (could be any member of public) in scientific research projects to solve real-world problems. Nowadays, information and communication technologies (ICT) have become the most important tools for conducting citizen science projects. There is a big challenge that many citizen science projects could not have enough people who are willing to voluntarily participate in and contribute to the citizen science projects. A research project I am working on is focusing on understanding existing and potential citizen science project participants' motivation to voluntarily participate in the project using qualitative research methods. Based on understanding participants\x92 motivations, we are going to provide guidelines for how to design a citizen science project that could attract more citizens using crowdsourcing techniques.   ",
  'priority': '1'},
 {'idea': "Studying persuasion in Crowd-Funding sites:\n\nWhat are the factors that persuade the \x91crowd\x92 to fund a crowd-funded project? Example: What makes a project pitched in a crowd-funding site like Kickstarter be successfully funded? What persuades the crowd to take the project to its funding goal? To explore this question, I propose to build a model to measure the relative importance of various factors driving a crowd-funding project\x92s funding success, where the factors could be: (1). project's funding goal, (2). duration by which it should be funded, (3). number of backers,  (4). category of the project (art, technology etc.), (4). language used by the entrepreneur to pitch his project, (5). Facebook, Twitter and other social network connections of the entrepreneur.  \nCan we find a link between  language and persuasive behavior? Can we harness this connection for constructive use of persuasion? Does the social network of the entrepreneur drive more backers to fund the project? \n\nI propose to scrape projects from a crowd-funded site (e.g.: Kickstarter) and build a statistical model. The model will have several factors as input features and it will predict the chances of the project being successfully funded.",
  'priority': '1'},
 {'idea': 'Idea 1- Human computation and entity matching for locations\nEntitypedia is a knowledge base which contains entities for locations, people, and events. Content is imported from different external sources. When a new location is introduced into Entitypedia, existing locations are reviewed to ensure no duplications. However, algorithms are not always able to decide whether several locations refer to the same place. Issues such as incorrect/incomplete coordinates and using single coordinates to identify large geographical extensions (e.g., lake, valley, mountain range) represent challenges for entity matching.\nDuring this workshop we aim at designing a workflow which combines automatic and human computation tasks to resolve entity matching for locations. We will contribute with a list of entities and metadata for each entity.',
  'priority': '1'},
 {'idea': 'Large-scale brainstorming tool\n\nThere are a few tools that support group brainstorming. However, what do we have for 100 busy people trying to coming up with something cool? A good example for this kind of need is OpenIDEO platform (http://www.openideo.com/) where anyone can propose concepts to a real-world problem for social good. I want to address the scalable issue in large-scale brainstorming. It calls for a system that support automatic thought-organization. We might want a system that gives suggestion or appropriate prompts to generate fantastic ideas, enabling a new participant to contribute without having to dive too deeply into the topic.\n\nThe scope of the project can range from studying existing platforms to identify design suggestions or implementing a prototype of the system.\n',
  'priority': '1'},
 {'idea': '    The metaphor of \x93human computation\x94 has directed the thinking of designers, leading many to view humans as a solution for the shortcomings of AI. If AI can\x92t solve it, artificial-artificial-intelligence can! Hurray! I can haz tagged pictures. OK, there\x92s more than labels - Soylent, CrowdForge, etc., have shown us how a crowd of lay people can produce expert-level work on tasks such as copywriting and new articles writing. And there\x92s been a spur of research looking at many aspects of this type of paid micro-task crowd work. That\x92s great, but: Is that what we should be after?     \n    I\x92d like to challenge us to start from the end, not the means. Why start with the limits of AI as the bar we aim at? Why not by asking how we can surpass the limits of human intelligence? Think about big challenges, so big that no expert can solve them alone. Perhaps not even several experts, working together. Think FoldIt. Think DuoLingo. Think Wikipedia. Think global warming. These should be the challenges we address. These should be the questions we ask. These should be the systems we strive to build. I think these challenges are far more important, and this kind of thinking will get us much more than asking whether tweaking some parameters would lead a crowd of 20 Turkers to produce a better paragraph (to set the record straight: I say this with all due respect and sincere appreciation of the pioneering work of Greg, Michael, Nikki and everyone else). This comes not to disregard any work, but as an attempt to focus attention and effort on what I think is a more desired avenue).\n    I\x92d love to provoke, debate, and take the risk of being bombarded with tomatoes (I hate tomatoes), with the goal of engaging at least some smart minds in thinking with me on the next big thing. My next idea (2) proposes one such concrete big problem which the crowd might help solve.\n',
  'priority': '1'},
 {'idea': "As we bring the crowd into more complex end-user applications, there are opportunities and challenges in enabling effective communication between requester and crowd. This can occur when the crowd is working on a complex  request, such as planning a trip or asking for advice on what to wear. In such situations, the crowd may need to elicit, understand, and act on the preferences, constraints, and private information known to the requester. The crowd thus need to not only reason about the task but also about the requester. \nThe requester to provide additional information and direction through the process of problem solving.\n\nIn particular, the crowd may need to ask questions to clarify concepts, and the requester may wish to interject in real time to provide clarifications, control the direction of the work, and in general, to function more as an active leader and collaborator in the interaction than a passive observer of eventual output. \n\nSome questions in this space include: what are effective modes of communication? How can information that is communicated be effectively pinned or organized so that all crowd workers can keep track of what's going on? What are the different interaction types (real time, notifications, crowd/system/requester initiated, etc) and how may interfaces need to be tailored to these types of interactions? It would be great to design some experiments around these questions, and to build a prototype of a novel application that leverages effective requester-crowd interaction.\n\n",
  'priority': '1'},
 {'idea': 'Study how users respond to crowdsourcing sensitive data (i.e. medication labels)',
  'priority': '1'},
 {'idea': 'I am interested in exploring automated methods for inferring varying worker expertise. While qualification tests have been used to find qualified workers, significant human effort is required for preparation and management of qualification process. In contrast, I\x92ve been investigating a matrix factorization method in order to find automatically infer experience. By treating a worker\x92s responses like user ratings in collaborative filtering, we identify worker expertise just like a recommendation engine learns user preferences. I would be interested in working with others to further develop this approach and/or integrate it with other techniques for identifying and modeling worker expertise.',
  'priority': '1'},
 {'idea': 'Due to the large numbers of people who will soon be retired, I think a project to motivate older adults to be workers, and even requesters, of crowdsourced tasks would be intriguing. ',
  'priority': '1'},
 {'idea': 'I\'m interested in the intersection of crowdsourcing and historical research. Can we develop "citizen historians" similar to citizen scientists? I find history an especially interesting domain because it combines the more objective methods of science with storytelling and subjective interpretation. There seem to be many ways to leverage the power of crowdsourcing: researching local or family history (which often gets less attention), locating and digitizing documents and artifacts distributed across the world, identifying and mapping old photos, collaborative curation or visualization of historical data, etc.',
  'priority': '1'},
 {'idea': 'Can crowdsourced work educate the worker in addition to performing a productive task? Imagine we have a large corpus of writing that needs to be checked for grammatical errors (for example, GRE writing submissions). Can proofreading this work simultaneously educate the worker? If so, could it lead to opinion change? What is the cost of changing an opinion? Does it work more or less effectively than directly paying the worker to read and comment on an argument?',
  'priority': '1'},
 {'idea': 'I would like to study and understand the impact of various social mediums on marketing products. For example would a company benefit more from advertising and marketing on  a text based medium such as twitter or an image based medium such as pinterest.',
  'priority': '1'},
 {'idea': "I'd like to explore for crowd work what iTunes did for music, i.e. can we encourage more ethical behavior by simply making it easier through technology to do the right thing. For music, this was making it easier to buy rather than steal digital music. For crowd work, it's paying wages compliant with national minimum wage laws in the worker's local region. \n\nImagine putting two simple pieces of approximate information together: Wikipedia's table of national minimum wages, and looking up a worker's location via IP address. We provide an intermediary between turk and the requestor. The worker indicates the maximum hourly pay rate he is willing to pay and we show the set of eligible countries for workers. When a worker accepts a HIT, we look up his location and set his pay appropriately if it's under the limit, or simply notify the worker the task is unavailable if the pay rate is below the minimum wage. We do this as an external HIT, priced per the cheapest country, with workers from higher paying countries being paid corresponding bonuses to match their national minimum wage rate. \n\nWe would focus on designing the system to be as easy to use as possible and survey requestors and workers (including feedback solicited in the interface itself) to collect qualitative feedback to guide design  revisions. There may be interesting past literature to survey about prior cases of technology and work crossing borders and what they suggest about the future of crowd pay.",
  'priority': '1'},
 {'idea': 'Diverse opinions are often a necessary component of collective intelligence, sometimes even more so than individual accuracy (Page, The Difference, 2007). But crowdsourced tasks often present data to workers in a single representation. Can we improve the results on tasks like visualization analysis by introducing variation in the format in which workers see the data visualized? Improvements could occur through a potential balancing of errors that might be achieved for visual analysis tasks where given graph formats can each bring systematic biases is one direction or another. The project would involve finding a good task for demonstration and demonstrating this over a case where the representation is kept the same across workers.',
  'priority': '1'},
 {'idea': 'Similar Scene Detection from Videos\n\nWhat if I only want to watch kiss scenes from movies? Touch downs from football games? Variations of Gangnam Style elevator guy parody scenes? The crowd might be able to detect similar scenes across multiple videos in a scalable and accurate way. Getting the mappings between scenes can open opportunities for interactively navigating a large video corpus.\n',
  'priority': '1'},
 {'idea': 'Crowd benchmarks: as we build "crowd-powered" systems, and invent new algorithms that use human computation, we often prototype on top of Mechanical Turk.  But MTurk isn\'t the only crowd, and may not be the right one for a system in the long run -- MobileWorks, CrowdFlower, oDesk, Facebook, Twitter, intra-organization groups may be better.  Thinking about a crowd as a computing platform, what are the right metrics for measuring crowd performance and behavior that will let us compare crowds, port a system from one crowd to another, and predict the performance and quality characteristics of an algorithm or system deployed on top of that crowd?  My hope for the weekend is to start to define a small set of metrics (e.g. spam rate, noise rate, response rate, latency, work rate) and a small set of benchmark tasks that can measure them on any crowd.',
  'priority': '1'},
 {'idea': 'From large scale conversation to collective action:  How can we help people become aware of civc themes in social media conversation on an extremely large scale, and then enable people to translate those conversations into meaningful collective action?  For this project,  I am imagining a simple tool built using Twitter that first helps people find others participating in a large scale topic, suggest hashtags, and then suggest discussion group, mailing list, or google doc for enabling more coordinated collective action.',
  'priority': '1'},
 {'idea': 'Benchmarking and demographics for crowdsourcing platforms. If we as a community develop a core set of tasks, we can finally begin comparing techniques and platforms as researchers develop them. As a side effect, we can start collecting demographic data from the workers (e.g., on MTurk) and publish those to other researchers so they can match on worker ID.',
  'priority': '1'},
 {'idea': "99 Designs Competitor -- 99 Designs gives ~$300 to the winner of a logo design contest, but people complain that all the hard-working designers who didn't win end up working for free. Can we create a better logo while paying everyone for their work? Perhaps we use $100 paying $5 to 20 designers for an idea/sketch of a logo, then we pay maybe $10 to crowdsource finding the best 5 sketches, and pay $20 to those people to refine their sketches, another $10 to crowdsource finding the best one, and then the remaining $80 to the winner to finalize their sketch.",
  'priority': '1'},
 {'idea': 'The goal of Idea 1 is to develop an interface to allow the requester to control the creativity of a crowd-based design process.\n\nDesign (or problem solving) can be viewed as a search through a design (or problem) space. Recent work  has shown how incorporating recombination in a crowdsourced design process can improve the creativity of the end result (Yu & Nickerson, 2011).  Yet "more creative" does not always imply "better." It may be possible to balance creativity with the fitness of the end-product using market levers that drive the crowd to either exploit a region of a design space or explore it more broadly. I have been developing an AMT prototype that employs differential payouts for recombination, improvement, and generation, and can adjust these dynamically. This prototype system is being used to generate solutions to problems in the climate change domain (for use in the Climate CoLab http://climatecolab.org). \n\nTwo immediate goals are 1) to develop an interface that allows a requester to manually guide the search via a set of levers and 2) create automated policies that generally lead to end-products with desired properties (e.g. more creative). By the time of the workshop, I will be in a position to run experiments to explore different search policies (1), and can guide other in developing interfaces to the system.',
  'priority': '1'},
 {'idea': 'The Micro-Engagement Project: \n\nA handful of crowd-based platforms for volunteering, civic participation, and humanitarian aid have succeeded wildly, and suggest that a wider variety of crowds can be engaged around the production of all sorts of civic and collective goods. I would like to recruit some bold and civic-minded workshop attendees to collaborate on a vision for a civic crowdsourcing project and/or design agenda. Ideally, we would identify a target problem as well as potential partner communities and organizations and generate a design, prototype, and evaluation protocol for a project (or two). Following the workshop, we could then use these materials to pitch our ideas to our imagined project partners after the workshop and pursue our vision.',
  'priority': '1'},
 {'idea': 'How to identify similar ideas?\n\nIn crowd innovation websites, such as Quirky and Threadless, a large number of ideas are submitted everyday and these ideas are presented through images, text and other media. As the number of the submitted ideas scales, it becomes more and more difficult to keep track of ideas. Once a new idea is submitted, it is hard to tell whether the similar idea has existed within the site or out on the market. Therefore, the challenge is how to identify the similarity of the ideas?  I would like to discuss this issue with the workshop participants, especially those with design, image and text processing background. \n',
  'priority': '1'},
 {'idea': 'Social science suggests that in many volunteer-based social computing systems, the presence of many consumers motivates contributors. For example, readers contribute to Wikipedia because they know many people read it.  But information about viewership is rarely very visible.\n\nHow can we design a workable system to make viewership (of wikis, lets say) more visible to contributors or potential participants? How might we test this experimentally -- in a real community -- to see how it shapes contribution patterns. Can we solve "social failures" (think market failures) and help align volunteer-driven supply with demand?\n',
  'priority': '1'},
 {'idea': "Constraints, Crowds, & Computers --- I'd like to look at constraint satisfaction problems where you need both human domain knowledge (to generate possible values for the variables and/or to evaluate constraints) and computers to guide the process so that a solution is found and so that it is found efficiently.  An simple example could be a design of a color palette: we want a solution where the overall palette is *harmonious* (a global constraint) and where certain pairs of colors, such as those used for text and background, have sufficient contrast (local constraints).  Human contributors suggest colors and evaluate constraints; the compute picks what node (i.e., color) to update next and how to prioritize constraints.  Computation aspect will draw on standard CSP and distributed CSP methods.   ",
  'priority': '1'},
 {'idea': 'Civic Media Crowdsourcing: How to leverage (some) people\'s intrinsic interest in bringing about change in their local communities and the world? For example, what kind of tools can one build for activists on the meatspace (i.e., Occupy) and "clicktivists" online?',
  'priority': '1'},
 {'idea': "Expertise is a scarce resource in crowd work. The current expertise-blind task-assignment practice makes this problem worse by assigning tasks to over-qualified or under-qualified workers. How can we efficiently and accurately match expertise level and task difficulty? Inspired by some computer-based standard tests (e.g. GRE) that adapt the difficult level of questions to how well the test taker has responded to previous questions. I'm interested in investigating mechanisms of adaptive task assignment to dynamically allocate expertise in a more efficient manner in crowdsourcing projects. Moreover, by making rewards proportional to the difficult level of tasks, I would also expect such mechanisms to incentivize crowd workers to improve their skills related to the work, and retain crowd workers for relatively long-term projects.",
  'priority': '1'},
 {'idea': 'What if a group of novices could perform physical actions like drawing or singing or creating a 3D model at a level beyond their individual expertise?  I propose a project in which multiple humans (a crowd) each contribute one physical action, such as singing a song or carving a 3D model (with a voxel modeler).  We would combine the inputs to produce a single, better-looking output.  With 3D models, we can align and average the voxels.  With songs, we can average or take the median of frequencies and amplitudes of the short-time-fourier-transform.  The output would appear as if a single human created it\x97rather than a composite\x97yet it will have been created collaboratively, surpassing the abilities of the individual members of the crowd, who may be unskilled or have motor disabilities.',
  'priority': '1'},
 {'idea': 'In open source software projects, software bugs are usually resolved through a decision making process. While group decision making is inherently complex, the remote and asynchronous nature of online discussions, coupled with the large number of participants further intensi?es the complexity. What are some ways that we can support this process? ',
  'priority': '1'},
 {'idea': "Real-time collaboration between crowd workers \xa0-- Typically crowd workers work alone and asynchronously. Individual work is then aggregated, to sum incremental and smaller task components to reach a desired goal. However, many types of work (such as that involving creativity) can benefit from receiving rapid feedback and playing off collaborators' ideas. Our project at the last crowdcamp demonstrated this,  by providing a realtime group image search engine. Participants at the workshop were able to quickly and incrementally riff on one anothers image searches, with rapid, fun and creative results. The positive results came from allowing participants to play off one another, in real time. I would like to expand on this by exploring what other types of work that might benefit from synergizing collaborators in realtime.  Further, even though Mechanical Turk may not have a suitable user base, tradition or platform for realtime collaboration, an important question is whether or not having unfamiliar and anonymous crowd workers collaborating on tasks is feasible, or desirable.",
  'priority': '1'},
 {'idea': 'Tuning the crowd to specific levels of description: When generating some types of answers, such as natural language labels, workers might need to agree on not only the description, but the level of generality used in the description. I would like to investigate ways to focus crowds on labeling at a specific level of abstraction. For example, in an image-labeling task where a worker is given a picture of a chair, we might want the \x91default\x92 or \x91natural\x92 label of "chair", or we might want a more specific label like \x93antique wooden arm-chair\x94 or a more general label like \x93furniture\x94, depending on the use case. This work has applications in image labeling, activity recognition, text classification, summarization, and more.',
  'priority': '1'},
 {'idea': "Spontaneous Creativity on MTurk:\nMturk may not be the best tool for eliciting creative work from the crowd, but it's one of the quickest methods, so perhaps we would like to figure out how to use it more effectively. If we also assume that creativity is something people want to practice more of in general (e.g. learning to draw/paint/write) maybe we can learn more about it by trying to spark creativity in Turkers. Some possible scenarios to study: drawing, writing/story-telling, designing/inventing (e.g. Yu's Cooks/Cobblers CHI paper). Some possible questions to ask: What makes a good prompt? Humor, specific constraints, lack of constraints, a set of related prompts? (Maybe lessons from improv apply here.) How does showing Turkers examples of other Turkers' work inspire them? What about having the build off each others' artifacts? How do we even effectively judge creative effort?\n",
  'priority': '1'},
 {'idea': "In particular for interactive systems (like the text editor Soylent) the crowd latency is still a problem because the systems are not really responsive. Earlier this year, we presented an allocation and pricing mechanism (CrowdManager) for human-task matchmaking in time constrained tasks which helped us to partially overcome these kind of problems in paid crowdsourcing. However, the recruitment process were/is still a problem because what we do is mainly to pre-recruit a large amount of crowd workers into a retainer (In particular, we use a variation of Michael Bernstein's Retainer model). In paid crowdsourcing, a possible solution would be to use an option-based payment and recruitment mechanism to solve this problem (mainly to reduce the costs). Therefore, I propose to think about new kind of recruitment mechanisms for  (paid and unpaid) crowdsourcing and come up with a team that could implement and evaluate them in 2013. ",
  'priority': '2'},
 {'idea': 'Genesis?\x94Genesis\x94 is a collaborative design platform for designers to work on product design using 3D printing technologies. 3D printing is revolutionizing product design by decentralizing design work previously done in big companies into distributed teams. Individual designers, e.g., usability specialists or artists, can print out 3-D models of the design to for evaluation from their own perspectives. \x93Genesis\x94 provides user interfaces and an application layer that combines 3D printer with other 3D technologies such as laser scanning, holographic displaying, and 3D motion capturing.  By allowing the users to create and interact with physical 3D prototypes, \x93Genesis\x94 builds up a collaborative platform for people with different specialties, such as engineering, industrial design and social work, to make their own contribution in designing process. With \x93Genesis\x94, we could also study how collective intelligence applies to this kind of online cooperation in the field of physical industrial design.  \n\n',
  'priority': '2'},
 {'idea': 'The second idea is to explore and understand the collaboration between scientists/administrators and science enthusiasts/citizens in citizen science projects, and then to see what kind of collaboration can increase the number of participants and their commitment to the citizen science projects. Usually, the number of scientists/administrators is significantly less than science enthusiasts/citizens. And the science enthusiasts/citizens\x92 background knowledge and experience could be very diverse. Given the limited number of scientists/administrators and the science enthusiasts/citizens\x92 diverse backgrounds, it seems unrealistic for scientists/administrators to collaborate with each individual science enthusiasts/citizens in citizen science projects in the same way. Our aim is to design ICTs to support this kind of collaboration, which is important and meaningful for the citizen science projects.',
  'priority': '2'},
 {'idea': 'Unraveling Deception in Social Media Using Crowd-Sourced Fact-Checker:\n\nThe motivation for this project idea came after closely following a recent political event \x96 the first presidential debate. Surprisingly, Reditt had turned into a real-time crowd-sourced fact-checker . Probing into the Reditt fact-checking conversations revealed some interesting findings. When posed with potential false statements, people responded with:\n1.\tSuspicion (did not know for sure, true?)\n2.\tAmusement (a laughable idea, LOL) \n3.\tDenial (this one is False, Worst lie of the night).\n4.\tAgreement (Yes, this is also true)\nI propose using natural language processing techniques to detect deception cues in social media and build on this understanding to predict human responses to other deceptive messages.\n\nAdditionally, I propose to develop interface affordances in current systems where users can report deception whenever they detect one. ',
  'priority': '2'},
 {'idea': 'Idea 2- Improving territorial data with social computation\nVisitTrentino is a tourism portal from Trentino region (Italy). It has a large amount of information on locations, events, and people. Although semantically related, most information is disconnected. The KnowDive group at the University of Trento has included large part of the available information on locations into semantic entities which can leverage the opportunities of the portal.\nHowever, incompleteness and incorrectness is an issue for some entities. Due to the high level of specificity, solving this issue requires knowledge of the territory.\nVisitTrentino has a very active community on its social channels. Participants comment, discuss and contribute in proposed challenges such as photography competitions of Trentino locations.\nOur proposal is to design a framework to involve the VisitTrentino community into a social computation project which would improve the quality of the knowledge base.',
  'priority': '2'},
 {'idea': 'TurkRecommender\n\nThe current interface on Amazon Mechanical Turk does not support matching workers with tasks that work for them and vice versa. Requesters usually have to come up with qualification tasks (which is not the best way to go in some cases). Looking from the workers\x92 point of view, things don\x92t look good either. There are some malicious requesters. Sometimes, there are types of tasks a worker prefers. They have to come up with task search strategies for themselves. Although there are tools that help turkers track requesters, searching for the right tasks can be more efficient.I am thinking about some recommending system like stumbleUpon, Pandora etc.For example, a worker can query for plan of doing variety of fun tasks that will pay them $3 for half an hour of work. A requester might be able to use such system by providing a more accurate information about their HITs and get (hopefully) happier, more willing turkers as a result.\n\nNote that I am using Amazon Mechanical Turk market as as main example but this idea should apply to other similar online job market as well.\n',
  'priority': '2'},
 {'idea': '2.\tAddressing the bottleneck of expertise in evaluating complex crowd-generated ideas (a broadly defined, yet more concrete project):\n\n    The collective-intelligence of crowds is increasingly used to generate ideas, plans, designs and predictions, for addressing various challenges \x96 from folding proteins to identifying galaxies, to urban planning. In some cases, evaluation of crowd inputs can be done by non-experts, or even automatically (e.g. FoldIt, reCAPTHCA). But in other scenarios, e.g. when the crowd produces a mass of complex plans for dealing with climate change, the evaluation process requires high levels of expertise in multiple domains \x96 a combination that might be rare even on global scale. For instance, evaluating such a plan might require assessments of technical feasibility, social and economic impacts, political factors and geophysical models. In such cases, current review procedures are much more akin to the academic review process: they are slow, inefficient, and not scalable, due to the scarcity of expertise. I began thinking and working on ways to mitigate that bottleneck, with the idea of combining crowds, semi-experts and experts in the process. This is a real project, done within the context of the Climate CoLab (climatecolab.org). If successful, some insights from this research should be relevant and applicable in many other domain of open innovation. By the time of the workshop I hope to have some preliminary pilot results.\n    At CrowdCamp I hope to engage with others interested in this problem (not limited to the climate domain), who would like to brainstorm various approaches, help design experiments, and run them. My next idea (3) proposes one concrete study I\x92m thinking about (though by Feb there may be many other ideas, and I\x92m also open to suggestions).\n',
  'priority': '2'},
 {'idea': 'For many of us, a good chunk of our day is spent on reading and writing emails. Each of us employ our own strategies for managing our inbox, but few of us seek help from others for processing, responding, and writing emails. \n\nThe proposal is to explore how a crowd can write and respond to emails on your behalf. In addition to raising technical challenges about how the crowd can make sense of the context and understand the requester\'s desires/goals to effectively help in this domain, there are interesting social questions around privacy and trust. \n\nMore generally, this raises a question about what role the crowd may have as an agent acting on your behalf: what happens when you can have many versions of "you" doing tasks as if they were "you," where each "version" may itself be made up of a crowd? What are the potential applications, technical challenges, and social issues surrounding this notion of replicating yourself through crowds?',
  'priority': '2'},
 {'idea': 'Support for real-time transcription of lectures by multiple people local and remote',
  'priority': '2'},
 {'idea': 'Effective interface design involves balancing various competing factors which impact the quality and cost of results, task efficiency, and satisfaction of users (impacting system adoption and retention). While this general theme has been studied in many domains, relatively little work has investigated interface design methodically in the specific domain of crowdsourced tasks for crowd workers. I would like to do setup and perform a variety of A/B experiments investigating a range of these design factors with particular regard to crowd work and crowd workers. While my own experience and expertise lies in designing crowd interfaces for the task of collecting relevance judgments for search evaluation, I\x92d also be interested in studying design factors across multiple crowdsourcing tasks to control for task and understand impact of factors across tasks.',
  'priority': '2'},
 {'idea': 'Currently on a marketplace such as Mechanical Turk you can select "skilled workers" for certain tasks. But skilled workers are expensive. How can we motivate skilled workers and expert crowds to work on projects indirectly related to them?',
  'priority': '2'},
 {'idea': "I'm interested in what principles of crowdsourcing are generalizable, and which are domain-specific. We have seen crowdsourcing applied to a huge variety of domains, from planning itineraries to shortening documents to creating movies and even beer recipes. My concern is that these findings will either be unnecessarily narrow (claims are limited to one domain, when in fact they're more general), or too broad (claims to be applicable to many domains, but in fact isn't). It's important to have this conversation or develop some kind of framework so we can identify the relevance of each paper in the growing literature.",
  'priority': '2'},
 {'idea': "Political engagement shares a number of common traits with what we typically think of as crowdsourcing in the CHI/CSCW communities. In each case, an individual makes a minor contribution which may feel more or less important to that individual, but when aggregated with the contributions of others results in more significant 'work'. In general, I'm very interested in combining research on crowdsourcing with research on political engagement. Are there lessons from one of these domains that are transferable to the other? Where does that break down?",
  'priority': '2'},
 {'idea': 'I would like to understand the affect of demographics and geo-location on defining social media values. Is there a difference between the cultures in how they perceive the social media interactions? Do they use it differently? Do they react to it differently?',
  'priority': '2'},
 {'idea': "One idea from last year had several express interest but didn't happen -- crowdsourced peer-review. How might we take a complex task like peer-review of research papers, with gold standard judgments from past papers we have reviews for, and design a workflow for workers to usefully / effectively contribute.  Besides being fun, we know the task domain well so are in very good position to do design and evaluation, and this represents a useful challenge task for continuing to raise the bar in terms of supporting complex work though creative and careful design. It also is a useful surrogate for similar content moderation and relevance assessment tasks for which a large volume of work is already being done, thus new proven methods could have significant practical impact on today's crowdsourcing marketplace.",
  'priority': '2'},
 {'idea': "Incentives in crowdsourced work - specifically, how does randomness in the function for judging work quality to assign a reward affect participation and the quality of work submitted for a crowdsourced assignment? In other works, assuming a requester with a set budget, would they do better to incentivize workers with a lottery scheme where a randomly chosen worker wins the full amount, a partially deterministic scheme where one's chances of winning the full reward increase with the quality of one's work, a fully deterministic scheme where the best quality work wins the full reward, or a flat scheme where each worker who submits reasonable quality work is paid the same amount? A controlled experiment is the most obvious means by which this research question could be answered.\n",
  'priority': '2'},
 {'idea': 'Delegating Information Consumption to the Crowd\n\nFor many knowledge workers suffering from information overload, there are too many things to read, watch, and follow up. Their reading list gets full, downloaded documents rapidly fill up the desktop, open tabs easily reach dozens, and bookmarks are monotonically increasing. While you do not have enough time to go over them, you also have the fear of missing out and falling behind. Can the crowd "consume" information for you to save your time? Can the crowd provide a coherent summary of an article? TED talk? Tutorial videos?\n',
  'priority': '2'},
 {'idea': "Low-fi prototyping for crowd systems: Paper prototyping is the classic technique for low-fidelity prototyping of single-user UIs.  Let's update paper prototyping for the crowd era.  Existing collaborative tools (Google Spreadsheets, Google Docs, OneNote, LucidChart, FlockDraw, Etherpad) can be repurposed with little or no coding to experiment with crowd-powered applications and workflows, to quickly discover usability problems and new design challenges.  What other tools can be brought into play, particularly for the nonprogramming participants of the design process?  What tools are missing and need to be developed?  What roles should the wizard-of-oz play?  How can we do an asynchronous prototype?  My hope for the weekend is to build a bunch of quick prototypes, try them out on other CrowdCampers and maybe remote crowds, and start to put together a prototyping toolbox and a handbook for using it.\n",
  'priority': '2'},
 {'idea': 'Groups net:  In my own research I am often surprised to encounter the ongoing important role mailing lists play in people\'s more complex communication/collaboration practices.   It is a technology that has changed little since the early \'90s, and yet is still prominent for group coordination.  Many of these mailing lists are public.  I have often thought they are "underleveraged" as a means for understanding the intersection of information and group collective behaviors.  What if we wrote a groups scraping tool that mapped a whole new world network of people and information, based on co-membership information?',
  'priority': '2'},
 {'idea': 'Flash organizations: dynamically creating teams of experts to tackle difficult problems that require multiple types of expertise. Possible platform: oDesk.',
  'priority': '2'},
 {'idea': 'Can we do something like TalentCourt (http://realgl.blogspot.com/2012/05/talent-court.html or http://apps.glittle.org/talentcourt#main) but for Programming? That is, can we come up with simple machine generated programming questions that people can solve, and then vote on the best solution?',
  'priority': '2'},
 {'idea': 'The goal of Idea 2 is to explore a framework for discovering good problem decompositions.\n\nA hurdle in using micro-task work is in figuring out how to split the problem up. One approach might be to delegate decomposition to the crowd (e.g. Kulkarni, Can, Hartman, 2012).  However, for complex problems, decomposition can actually lead to the discovery of interdependencies among sub-problems that in turn necessitate re-decomposition.  This suggests that crowdsourcing decomposition might benefit from iteration between decomposition, integration and evaluation.  \n\nI have begun designing an algorithm around this idea, which is best framed as a search through a hierarchical space of decompositions for a given problem.  Any particular decomposition is hierarchical--at the first level, a problem is broken into subproblems, subproblems are further broken down, and so forth. Any subproblem can itself be considered a problem space with more than one possible decomposition. The crowd moves through this space by 1) decomposing a level, 2) proposing sub-solutions using the decomposition 3) evaluating sub-solutions 4) integrating sub-solutions and revaluating.  Various policies might then be used to guide the search through multiple levels.\n\nAs with Idea 1, I should be in a position to begin exploring different search policies for various problem domains by the time the workshop happens.',
  'priority': '2'},
 {'idea': 'Crowds With a Purpose: \n\nCertain kinds of "idle time" we encounter in our everyday lives can feel completely debilitating, disempowering, and dehumanizing. The hours that you\'re waiting to be seen in an emergency room; those empty minutes when you\'re stuck on a delayed bus or train; even just sitting in stop-and-go traffic on your way home from work. Can we develop, prototype, and design evaluations for crowd-based engagement platforms that can us help to retain or recover a sense of purpose, meaning, and connection during these otherwise empty, unpleasant and often isolating moments in our lives?',
  'priority': '2'},
 {'idea': 'Visioning social ideation\n\nThis is a proposal of writing a paper on social ideation. Social ideation is a phenomenon that crowds generate ideas and the best ideas get developed and manufactured once they are selected. Examples include Quirky, Threadless, AHHHA, etc. Social ideation is a new way of leveraging internet and the crowd, following social networking(Facebook), social media(twitter) and social gaming(Zynga). Social ideation explores the crowd\x92s ability of doing creative work instead of simple work. I believe a theoretical paper will make a big contribution to this field and open a series of valuable research questions. \n',
  'priority': '2'},
 {'idea': 'We know lots about Wikipedia and quite a bit about free/libre open source software. We know a bit about a few different remixing platforms. A theoretical framing based on volunteer labor and low-transaction costs ties these all together but every paper on any one of these communities ends with claim that future work will explore generalizable of results.\n\nHow we would actually go about doing this? Can we design and build a sort of meta-analysis? Can we design a simple test on a widely hypothesized mechanism (e.g., reputation-based effects promoting high quality contributions) that can bested cleanly and comparably across a large number of different communities?',
  'priority': '2'},
 {'idea': 'Optimization, Constraints, Crowds, & Computers --- To make it more fun, let\'s turn it into a constrained optimization problem: the solution not only has to be harmonious and satisfy contrast constraints, but let\'s optimize for some subjective effect.  For example, we can strive to find a color palette that\'s most "active" or most "sophisticated" or that best fits the word "electrified".  Here we can try to test out a variety of basic optimization strategies: we can start with local search (alternating between improving the objective and repairing constraints), then see if we can articulate an admissible heuristic so that we can use one of the heuristic search methods, look for the best greedy approach, etc.',
  'priority': '2'},
 {'idea': 'Rube Goldberg Crowdsourcing Machine: A completely crowd-funded, crowd-defined, and crowd-implemented self-emergent project for the lulz. Step 1: Raise money on Kickstarter. Step 2: Use the money to fund MTurkers to decide what crowdsource tasks will be implemented on the meatspace. Step 3: Hire TaskRabbiters to carry out the project in the physical world. The research challenge here is how to design the project to be resilient and attractive to crowd-donors. The project would be the embodiment of crowdsourcing \x97 it will both celebrate it and problematize it.\n',
  'priority': '2'},
 {'idea': "Many crowd-sourcing projects ask workers to cognitively process information in text or multimedia formats, such as classifying products, tagging videos, and recognizing images. As a by-product of those tasks, the worker learns something about the content on which his/her task is based. Can we deliberately design tasks to optimize for the purpose of disseminating important information to populations that normally wouldn't care or be aware of such information? For example, we might be able to disseminate information about healthy eating habit to crowd workers by asking them to classify food into healthy food or unhealthy food based on specified nutrition criteria. I foresee two challenges in such repurposing of crowdsourcing: 1) How can we measure the learning outcome of getting crowd workers exposed to the task content? 2) How can we identify and design tasks that optimize crowd-sourcing's potential as an information dissemination channel but in the same time retain its primary purpose of delivering value to the job requester?",
  'priority': '2'},
 {'idea': 'Can a crowd of random Internet users collaborate to solve puzzlehunt puzzles effectively, such as those from the MIT Mystery Hunt, the Post Hunt, DASH, or Midnight Madness?  Internet users can be given a simple threaded chat system to discuss puzzles.  We can modify the chat system to perform experiment on brainstorming and puzzle solving.  For example, I propose to address the problem of \x93design fixation\x94 by hiding threads in the chat system from some portion of the users (and revealing them after a delay) to prevent users from fixating on an idea.',
  'priority': '2'},
 {'idea': 'How can we compute the equivalent of the KLOUT score for expertise within a single community? How can we balance between content, activity, and peer votes? Should we make the algorithm transparent?',
  'priority': '2'},
 {'idea': "Social Learning between workers -- an important way that we learn how to be successful is from observing how others achieve success in  their work: what techniques they use in their tasks, how they allocate their time, and what subtasks they select. However, crowd workers most often work alone, working on many different, short lived tasks. This means that crowd workers don't have the opportunity to learn from others. I would like to explore how we could summarize and present overviews of successful crowd worker behavior, so that other workers can learn how they could improve their performance. \n\nFor example, imagine a HIT for a turker to write a paragraph describing a photograph. We would capture the behavior of a certain number of workers in completing the task. We would then have human raters score the quality of all the created paragraphs. We would select the highest scoring paragraphs as the gold standard. Then we would create summaries of worker behaviour in creating these gold std. paragraphs. These summaries might include how much time was spent on the task overall, how much time was spent on an original draft, how much time was spent editing the drafts, the avg. length of the best paragraphs, how many words per minute the top workers achieved in completing the tasks, etc. These summaries would then be made available to workers while they complete their work, and allow them to compare their behavior in the task to the behavior of those workers who provided the gold std. paragraphs. We could then measure if the overall productivity and success of workers improves by viewing these simple summaries of gold std. behaviour as feedback. I believe this example describes a project that is feesible to achieve within the two days of the workshop, and would provide valuable findings about how turkers could positively influence and learn from one another. ",
  'priority': '2'},
 {'idea': 'Crowd acting: What is the best way to elicit answers from the crowd that conform to a given role, rather than each worker\x92s true beliefs or identity? Systems such as Chorus has investigated having a natural dialogue with the crowd, but can we get a crowd to accurately and consistently respond as if they were a famous historical figure? A sick patient? \x85Santa Claus? These examples hint at significant potential for educational and entertainment value. This project would focus on incentive mechanisms and communication methods that allow workers to agree on the traits of a character being portrayed.',
  'priority': '2'},
 {'idea': 'Instead of crowdsourcing data collection (of image labels, ratings, pictures of objects around the world, whatever) as a separate phase from the processing of that data, what if the data collection and data processing were more _unified_ and a user got feedback about the usefulness of their data right after they made a contribution?\n\nOne possible manifestation of this is to simultaneously collect data for a facial expression recognition system AND train that system by having users take pictures of themselves making an expression, have the system make a guess, and then have the user validate that guess or possibly even validate/invalidate sub-hypotheses that the system makes. Maybe the system says "I think you are smiling because of Feature X and Feature Y" and the user says, "Yes, I am smiling, but Feature Y is irrelevant for making that judgement."\n',
  'priority': '2'},
 {'idea': 'This last idea is not as concrete as idea 1 & 2. In my vision, the "global brain" (the crowd) should be used to tackle global and everyday problems. Therefore, I\'m fascinated by projects like MIT\'s Climate CoLab where they try to solve Climate problems using the crowd. It would be interesting to come up with ideas to support problem-solving processes for huge global problems like poverty, economical or political crises, efficient use of local energy resources etc. by using the crowd (whereby "using" could mean that we use the crowd as a problem solver or crowdsourced work could be a part of the problem-solving itself). An idea would be that we come up during the workshop with a project for addressing one of the problems and then implement the project in 2013 with an international team (consisting of the workshop participants and interested people). I think this would be really fascinating, could have an huge impact for society, and would foster the collaboration between researchers in our field. In particular, I think also this could be an opportunity to foster the collaboration between the European and American human computation community. ',
  'priority': '3'},
 {'idea': '3.\tMicro competitions between teams of workers, vs. nominal groups: how closely can quantitative expert evaluations be approximated without using experts? (a concrete study)\n    \n    A critical aspect of any proposed plan for addressing climate change (whether a policy, a social action, or a technical solution) in the Climate CoLab, is the projected degradation of CO2 emissions if it were to be implemented. Experts use simulation software to make such assessments. Nowadays, there are also simulators and calculators freely available online. Can crowds use them and provide assessments that are on par with those of the experts?\nI\x92d like to try several different modes of work: \n    a.\tmechanical aggregation of crowd estimates. That seems like the basic obvious baseline, but there\x92s a catch: proposed plans can be long to read, and it may not be clear from the proposal how to calculate the associated degradation in CO2. We will probably first need to break it into paragraphs, and ask members of the crowd to identify relevant places in the text that seem relevant for making assessments.\n    b.\tAs an alternative, I\x92d like to create a competition between teams, and let them \x91break their head\x92. A team can do more than individual members \x96 e.g. it can break a long plan itself and divide labor. This can helps with a known hurdle in MTurk, where people don\x92t want to spend too much time on a task. Teams would receive a proposal (plan) and asked to assess CO2 emission degradation for that plan. A team whose assessment is closer to that of the experts (we\x92ll have those prepared in advance) wins a bonus. \n    I\x92d like to compare those two approaches on a number of dimensions, both \x91hard\x92 and \x91soft\x92. There are many parameters and configurations to think about (e.g. \x91self-managed\x92 vs. \x91managed\x92 teams), and some design challenges related to the competitions. But there are reasons to believe this approach can get us good results.\nAlthough this study is proposed in a specific context, insights from it should be transferrable to many other settings.\n',
  'priority': '3'},
 {'idea': 'A self-sustained peer feedback system\n\nI am interested in building a self-sustained peer-feedback system.  Being self-sustained means the crowd can help each other improve their work and learn new knowledge. My group has done some work on peer feedback and observed some interesting phenomenon. I would like to start from there and consider building a system allowing the crowd to provide high-quality feedback to their peer workers. Here is the basic idea:  a worker\x92s solution to a task will be reviewed by their peers. The original worker will be presented with the reviews and hopefully improve their work. There are  many challenges, for example, how to validate the peer feedback and how to assign work to reviewers. Experiments and system designs can be discussed and implemented during the two-day workshop.\n',
  'priority': '3'},
 {'idea': 'An experiment that examines whether introducing beneficial "difficulties" can improve the quality of work in crowdsourced tasks that involve reading or analysis of some presented information. The intervention I have in mind is based on studies that show harder-to-read fonts can improve comprehension (e.g., Daniel Oppenheimer\'s work), though other possibilities exist. For instance, requiring works to submit unanswered questions at key parts of a difficult task might also lead to observable improvements, as generating questions that one wants answered often shows the person where there are gaps in their own understanding of some information. ',
  'priority': '3'},
 {'idea': 'Build crowdsourcing tools for porting health data',
  'priority': '3'},
 {'idea': 'Can a small, online crowd increase productivity for creative tasks by watching and intervening in real-time?  I would like to try a Pictionary-like interface where a primary user working in a tool like Photoshop is watched by a crowd.  The crowd is constantly guessing what the user is doing, and the user sees the guesses heads-up display.  If and when a correct guess is made, the user chooses it, and the crowd is allowed to intervene and take over the operation.  (Amenable operations would need to be parallelizable in time, such as creating design variations, or parallelizable in space, such as painting color onto a black-and-white sketch.',
  'priority': '3'},
 {'idea': 'Crowd training/priming - recently work by Bernstein et al. has shown that work can be completed in near real time by keeping workers on retainer (i.e., paying them to be available for potential work tasks). It is assumed that while waiting for work, crowd workers are off doing other tasks. However, could this time be used to prepare workers for high performance work? Could they be paid to train or complete tasks that encourage them to increase their speed and quality of work? Where would these training tasks come from? Could the promise of increased pay be used as an incentive for workers to train up to target levels of performance?\xa0(This was an idea I submitted to the previous CrowdCamp, but I still like it).',
  'priority': '3'},
 {'idea': 'Crowdsourcing Creative Work: Create a creativity broker across social media platforms that leverages the existing communities that exist on sites like DeviantArt, YouTube, and other to create artistic mashups at large scale. ',
  'priority': '3'},
 {'idea': 'Exploring Broader Design Space with Crowd\n\nRather than rely on converging agreement from the crowd, how can we instead leverage the divergent nature of crowd work? Can we enable the crowd to explore a wide design space while still maintaining the quality? What workflow or system designs encourage the crowd to explore less-populated space? How can we tell the difference between noise, spamming, randomness, novelty, and uniqueness? Small experiments might be exploring ways to encourage the crowd to select less common options from simple multiple choice questions. The natural next step would be to encourage diversity in an open-ended setting. \n',
  'priority': '3'},
 {'idea': 'I would also like re-propose one of my ideas from last year: growing the space of mobile crowdwork. A "builder" version of this idea could be an iPhone or Android library to either: 1) create a mobile interface for performing existing MTurk tasks (perhaps identifying a subset most amenable to completion via a mobile interface), or 2) make it easier for requestors to create "mobile-compliant" HITs.  The intended distinction between these would be 1) emphasizes scraping existing HIT interfaces while 2) emphasizes creation of new HITs with conforming interfaces. An alternative, "studier" form of this idea would be surveying the marketplace and a sample of common tasks, and comparing the interface/interaction requirements of these tasks vs. the capabilities of iPhone/Android devices and hypothesizing potential sweet spots as well as roadblocks and limitations. How much current crowd work could be done on mobile, and what do future trajectories look like given the evolving nature of both crowd work and mobile devices?  How do these device capability trajectories look different in developing economies?\n\nIdeas 4, 5, .. (no separate form for more): 4) the intersection of online education and crowd work (training and assessing workers, learning by doing, generalizations of duolingo), 5) mining and exploiting worker expertise to increase skilled crowdwork, 6) the crowd dating site suggested last year where the crowd suggests possible pairings. Idea 7 is specific to computer vision but I don\'t know if anyone there will have this background.',
  'priority': '3'},
 {'idea': 'Idea3- Ethical implications of crowdsourcing studies\nCrowdsourcing is sometimes considered a panacea for many human computation tasks. As the number of institutions using crowdsourcing in industry and academia grows, the need for establishing ethical guidelines becomes more evident. Both requesters and contributors are independently dealing with emerging issues. This is exemplified by the extensive literature on quality assurance methods in crowdsourcing studies and by Turker Nation forum where Amazon Mechanical Turk contributors share their experiences and worries in "turking". During this workshop we would like to gather ethical dilemmas encountered by participants and formalize them to share with the community. We will contribute with a list of related issues identified in Turker Nation forum.',
  'priority': '3'},
 {'idea': 'Information seeking behavior reflects multi-dimensional aspects of human nuance on information needs and uses. It varies from confirming factual information to asking sophisticated information. However, it is challenging to capture and collect a series of user information seeking behaviors for academic research. While some commercial web search engines keep track of user\x92s behaviors via logging for their own research, most academic researchers are struggling with collecting the information except in small-scale laboratory settings. For this problem, I would like to come up with a crowdsourcing based information collection system which collates information seeking behaviors generated by crowd workers. I would be interested in working with others to design the system and /or integrate other factors for modeling information seeking behaviors.',
  'priority': '3'},
 {'idea': 'Interaction between experts and non-experts within crowd\n\nThis idea has not taken a solid form yet. I have been thinking about interaction between experts and non-experts in a crowd-based system for sometime. Some questions that come up: How do we identify experts? Should the system treat them differently? How should  experts and non-experts interact with each other? How should we distribute the tasks? What about a crowd with people of different expertise? Can a non-expert become an expert? I would be interested into looking at any data or coming up with a system that can help us answer these questions.\n',
  'priority': '3'},
 {'idea': 'Mind Reading for Better Personalization --- when you interact with most recommender systems, these systems fail to provide good personalized suggestions until the user provides at least a few dozen ratings.  Could the crowds be used to "read the mind" of the user based on a small number (say half a dozen) of examples and then make further ratings on behalf of the user?  Thus the crowd would serve as a temporary replacement for the machine learning to create more examples that the real machine learning could use to build an accurate model of the user\'s preferences.  I\'ve tried it with art, and it didn\'t work -- people weren\'t able to predict other people\'s taste for art.  But how about fashion, foods, or other less esoteric domains?',
  'priority': '3'},
 {'idea': 'Prototyping techniques for social computing systems. How do you design and perform early user tests of systems that need entire social networks to function and have emergent dynamics? ',
  'priority': '3'},
 {'idea': "Superstitions in peer teaching: Q&A sites like StackOverflow, or student help forums like Piazza or Coursera/edX, may be susceptible to the spread of misinformation -- misconceptions, superstitions, false beliefs.  Or do the reputation and voting systems of these systems keep it in check compared to other social networking sites (and real-world social networks)?  Let's use open data sets (like StackOverflow) to study how misconceptions form, propagate, and resolve themselves.  For the camp, I propose that we tackle some case studies qualitatively and use them to develop a strategy for large-scale quantitative analysis (probably subsequent to the camp). ",
  'priority': '3'},
 {'idea': "The collaborative, authored visual wiki network.   As someone who frequently organizes professional events, I know a lot about who's who and how they should be connected before the event.  I know that same information is in the minds of others as well, and have often wished there was a collaborative wiki network, where we could place individuals, groups, and interests as nodes, with implicit or explicit, actual or recommended connections as edges.  As a collaborative wiki, anyone could go in, add people, add groups, add connections, move connections around, and so forth.  I believe the CSCW community would be a good starting point -- we could create a simple application where people could start adding people during the event.",
  'priority': '3'},
 {'idea': 'The goal of Idea 3 is to develop approaches to generating solutions for the Climate CoLab (http://climatecolab.org).\n\nThe Climate CoLab (http://climatecolab.org) is a site designed to crowdsource solutions to climate change. This is a challenging endeavor, and we have begun to consider how to harness crowds outside of the CoLab community itself to develop solutions.\n\nThis workshop would provide an excellent opportunity to brainstorm about how best to use micro-task crowds in the CoLab, and maybe even implement some of these strategies. One challenging target problem in this domain is how to inject the necessary expertise into crowd-driven processes for solution creation and evaluation.  Could we use turkers to identify where and what type of expertise is needed? Could we identify and connect to other systems that have the necessary expertise?  Could we do a better job of identifying and leveraging existing expertise within the crowd? \n\nThis is a fairly open-ended idea, and I would be interested in exploring any possibilities within this space. There may be an opportunity to use the CoLab framework if it might in some way support the workshop (for instance, as an experimental platform).',
  'priority': '3'},
 {'idea': 'There are a lot of crowdsourcing platforms and services available online. The third idea is to investigate what kind of crowdsourcing platforms and services can be used in citizen science projects and will be most beneficial to citizen science projects.  ',
  'priority': '3'},
 {'idea': 'What are some ways that we can support researchers to come up with new and innovative research ideas? Researchers usually do not brainstorm research ideas with their peers working in the same research area. However, they may feel safe to share their ideas with researchers in a different field of research. Can we create an online platform to support brainstorming ideas among researchers from different fields of research? \n',
  'priority': '3'},
 {'idea': "What would a crowdsourcing or social computing system look like if it was designed t be used, be enjoyed, and lead to productive outcomes for, the large majority of the world's population who cannot read or write?",
  'priority': '3'},
 {'idea': 'Will crowdsourcing replace human labor? What is the level of things that can be crowdsourced now and can be in the future and how will this impact the culture of the workplace of the future?',
  'priority': '3'},
 {'idea': 'With a Little Help From My Crowd (alt: Lend A Helping Crowd): \n\nLife is filled with annoying little tasks you (and only you!) can do, but that would be more pleasant to forget, avoid, or ignore: Take your pills, take out the trash, pay that bill, pick up the kids at school, call your Aunt Phyllis on her birthday, etc. Even when services like Task Rabbit have made it easier for you to hire very part-time help for some things, there are still plenty of aggravating little items on your to-do list that you simply cannot pass off to someone else. For this type of task, I would like to recruit collaborators to design, prototype, and evaluate the impact of the "With a Little Help From My Crowd" system. Through With a Little Help From My Crowd, you can find a volunteer (or even several volunteers!) to provide you with reminders, friendly encouragement, supportive check-ins, and/or kudos and congratulations once you\'ve taken care of that nagging thing that you couldn\'t or didn\'t want to finish.',
  'priority': '3'},
 {'idea': "Writing is a fundamental skill, yet many people don't write well. While some problems in writing are related to grammar and syntax, more often than not problems reveal potential flaws in the organization of ideas and in logical reasoning. The proposal is to explore how we may design online systems and communities that help people learn how to write and make logical arguments. \n\nMuch like DuoLingo, the process of learning may involve actual content where students' work may help to improve it. Over the course of the workshop we may aim to sketch out a design of the system, as well as to implement prototypes of a few tasks, lessons, and interfaces.",
  'priority': '3'}
  ]
